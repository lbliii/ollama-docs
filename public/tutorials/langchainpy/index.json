{
    "id": "545bd5b72d44b172679bfcc7ceef04f3",
    "title": "Langchainpy",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/langchainpy/",
    "relURI": "/tutorials/langchainpy/",
    "body": " Using LangChain with Ollama in Python # Let\u0026rsquo;s imagine we are studying the classics, such as the Odyssey by Homer. We might have a question about Neleus and his family. If you ask llama2 for that info, you may get something like:\nI apologize, but I\u0026rsquo;m a large language model, I cannot provide information on individuals or families that do not exist in reality. Neleus is not a real person or character, and therefore does not have a family or any other personal details. My apologies for any confusion. Is there anything else I can help you with?\nThis sounds like a typical censored response, but even llama2-uncensored gives a mediocre answer:\nNeleus was a legendary king of Pylos and the father of Nestor, one of the Argonauts. His mother was Clymene, a sea nymph, while his father was Neptune, the god of the sea.\nSo let\u0026rsquo;s figure out how we can use LangChain with Ollama to ask our question to the actual document, the Odyssey by Homer, using Python.\nLet\u0026rsquo;s start by asking a simple question that we can get an answer to from the Llama2 model using Ollama. First, we need to install the LangChain package:\npip install langchain\nThen we can create a model and ask the question:\nfrom langchain_community.llms import Ollama ollama = Ollama( base_url=\u0026#39;http://localhost:11434\u0026#39;, model=\u0026#34;llama3\u0026#34; ) print(ollama.invoke(\u0026#34;why is the sky blue\u0026#34;)) Copy Notice that we are defining the model and the base URL for Ollama.\nNow let\u0026rsquo;s load a document to ask questions against. I\u0026rsquo;ll load up the Odyssey by Homer, which you can find at Project Gutenberg. We will need WebBaseLoader which is part of LangChain and loads text from any webpage. On my machine, I also needed to install bs4 to get that to work, so run pip install bs4.\nfrom langchain.document_loaders import WebBaseLoader loader = WebBaseLoader(\u0026#34;https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\u0026#34;) data = loader.load() Copy This file is pretty big. Just the preface is 3000 tokens. Which means the full document won\u0026rsquo;t fit into the context for the model. So we need to split it up into smaller pieces.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) Copy It\u0026rsquo;s split up, but we have to find the relevant splits and then submit those to the model. We can do this by creating embeddings and storing them in a vector database. We can use Ollama directly to instantiate an embedding model. We will use ChromaDB in this example for a vector database. pip install chromadb\nfrom langchain.embeddings import OllamaEmbeddings from langchain.vectorstores import Chroma oembed = OllamaEmbeddings(base_url=\u0026#34;http://localhost:11434\u0026#34;, model=\u0026#34;nomic-embed-text\u0026#34;) vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed) Copy Now let\u0026rsquo;s ask a question from the document. Who was Neleus, and who is in his family? Neleus is a character in the Odyssey, and the answer can be found in our text.\nquestion=\u0026#34;Who is Neleus and who is in Neleus\u0026#39; family?\u0026#34; docs = vectorstore.similarity_search(question) len(docs) Copy This will output the number of matches for chunks of data similar to the search.\nThe next thing is to send the question and the relevant parts of the docs to the model to see if we can get a good answer. But we are stitching two parts of the process together, and that is called a chain. This means we need to define a chain:\nfrom langchain.chains import RetrievalQA qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever()) qachain.invoke({\u0026#34;query\u0026#34;: question}) Copy The answer received from this chain was:\nNeleus is a character in Homer\u0026rsquo;s \u0026ldquo;Odyssey\u0026rdquo; and is mentioned in the context of Penelope\u0026rsquo;s suitors. Neleus is the father of Chloris, who is married to Neleus and bears him several children, including Nestor, Chromius, Periclymenus, and Pero. Amphinomus, the son of Nisus, is also mentioned as a suitor of Penelope and is known for his good natural disposition and agreeable conversation.\nIt\u0026rsquo;s not a perfect answer, as it implies Neleus married his daughter when actually Chloris \u0026ldquo;was the youngest daughter to Amphion son of Iasus and king of Minyan Orchomenus, and was Queen in Pylos\u0026rdquo;.\nI updated the chunk_overlap for the text splitter to 20 and tried again and got a much better answer:\nNeleus is a character in Homer\u0026rsquo;s epic poem \u0026ldquo;The Odyssey.\u0026rdquo; He is the husband of Chloris, who is the youngest daughter of Amphion son of Iasus and king of Minyan Orchomenus. Neleus has several children with Chloris, including Nestor, Chromius, Periclymenus, and Pero.\nAnd that is a much better answer.\n",
    "tags": []
}