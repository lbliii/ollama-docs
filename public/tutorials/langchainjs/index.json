{
    "id": "038459ec4912cf57382c9cc466df6e99",
    "title": "Langchainjs",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/langchainjs/",
    "relURI": "/tutorials/langchainjs/",
    "body": " Using LangChain with Ollama using JavaScript # In this tutorial, we are going to use JavaScript with LangChain and Ollama to learn about something just a touch more recent. In August 2023, there was a series of wildfires on Maui. There is no way an LLM trained before that time can know about this, since their training data would not include anything as recent as that. So we can find the Wikipedia article about the fires and ask questions about the contents.\nTo get started, let\u0026rsquo;s just use LangChain to ask a simple question to a model. To do this with JavaScript, we need to install LangChain:\nnpm install langchain Copy Now we can start building out our JavaScript:\nimport { Ollama } from \u0026#34;langchain/llms/ollama\u0026#34;; const ollama = new Ollama({ baseUrl: \u0026#34;http://localhost:11434\u0026#34;, model: \u0026#34;llama3\u0026#34;, }); const answer = await ollama.invoke(`why is the sky blue?`); console.log(answer); Copy That will get us the same thing as if we ran ollama run llama3 \u0026quot;why is the sky blue\u0026quot; in the terminal. But we want to load a document from the web to ask a question against. Cheerio is a great library for ingesting a webpage, and LangChain uses it in their CheerioWebBaseLoader. So let\u0026rsquo;s install Cheerio and build that part of the app.\nnpm install cheerio Copy import { CheerioWebBaseLoader } from \u0026#34;langchain/document_loaders/web/cheerio\u0026#34;; const loader = new CheerioWebBaseLoader(\u0026#34;https://en.wikipedia.org/wiki/2023_Hawaii_wildfires\u0026#34;); const data = await loader.load(); Copy That will load the document. Although this page is smaller than the Odyssey, it is certainly bigger than the context size for most LLMs. So we are going to need to split into smaller pieces, and then select just the pieces relevant to our question. This is a great use for a vector datastore. In this example, we will use the MemoryVectorStore that is part of LangChain. But there is one more thing we need to get the content into the datastore. We have to run an embeddings process that converts the tokens in the text into a series of vectors. And for that, we are going to use Tensorflow. There is a lot of stuff going on in this one. First, install the Tensorflow components that we need.\nnpm install @tensorflow/tfjs-core@3.6.0 @tensorflow/tfjs-converter@3.6.0 @tensorflow-models/universal-sentence-encoder@1.3.3 @tensorflow/tfjs-node@4.10.0 Copy If you just install those components without the version numbers, it will install the latest versions, but there are conflicts within Tensorflow, so you need to install the compatible versions.\nimport { RecursiveCharacterTextSplitter } from \u0026#34;langchain/text_splitter\u0026#34; import { MemoryVectorStore } from \u0026#34;langchain/vectorstores/memory\u0026#34;; import \u0026#34;@tensorflow/tfjs-node\u0026#34;; import { TensorFlowEmbeddings } from \u0026#34;langchain/embeddings/tensorflow\u0026#34;; // Split the text into 500 character chunks. And overlap each chunk by 20 characters const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 20 }); const splitDocs = await textSplitter.splitDocuments(data); // Then use the TensorFlow Embedding to store these chunks in the datastore const vectorStore = await MemoryVectorStore.fromDocuments(splitDocs, new TensorFlowEmbeddings()); Copy To connect the datastore to a question asked to a LLM, we need to use the concept at the heart of LangChain: the chain. Chains are a way to connect a number of activities together to accomplish a particular tasks. There are a number of chain types available, but for this tutorial we are using the RetrievalQAChain.\nimport { RetrievalQAChain } from \u0026#34;langchain/chains\u0026#34;; const retriever = vectorStore.asRetriever(); const chain = RetrievalQAChain.fromLLM(ollama, retriever); const result = await chain.call({query: \u0026#34;When was Hawaii\u0026#39;s request for a major disaster declaration approved?\u0026#34;}); console.log(result.text) Copy So we created a retriever, which is a way to return the chunks that match a query from a datastore. And then connect the retriever and the model via a chain. Finally, we send a query to the chain, which results in an answer using our document as a source. The answer it returned was correct, August 10, 2023.\nAnd that is a simple introduction to what you can do with LangChain and Ollama.\n",
    "tags": []
}