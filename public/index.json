[{
    "id": "00e71df22262087fd8ad820708997657",
    "title": "API",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/api/",
    "relURI": "/api/",
    "body": " API # Endpoints # Generate a completion Generate a chat completion Create a Model List Local Models Show Model Information Copy a Model Delete a Model Pull a Model Push a Model Generate Embeddings Conventions # Model names # Model names follow a model:tag format, where model can have an optional namespace such as example/model. Some examples are orca-mini:3b-q4_1 and llama3:70b. The tag is optional and, if not provided, will default to latest. The tag is used to identify a specific version.\nDurations # All durations are returned in nanoseconds.\nStreaming responses # Certain endpoints stream responses as JSON objects and can optional return non-streamed responses.\nGenerate a completion # POST /api/generate Copy Generate a response for a given prompt with a provided model. This is a streaming endpoint, so there will be a series of responses. The final response object will include statistics and additional data from the request.\nParameters # model: (required) the model name prompt: the prompt to generate a response for images: (optional) a list of base64-encoded images (for multimodal models such as llava) Advanced parameters (optional):\nformat: the format to return a response in. Currently the only accepted value is json options: additional model parameters listed in the documentation for the Modelfile such as temperature system: system message to (overrides what is defined in the Modelfile) template: the prompt template to use (overrides what is defined in the Modelfile) context: the context parameter returned from a previous request to /generate, this can be used to keep a short conversational memory stream: if false the response will be returned as a single response object, rather than a stream of objects raw: if true no formatting will be applied to the prompt. You may choose to use the raw parameter if you are specifying a full templated prompt in your request to the API keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m) JSON mode # Enable JSON mode by setting the format parameter to json. This will structure the response as a valid JSON object. See the JSON mode example below.\nNote: it\u0026rsquo;s important to instruct the model to use JSON in the prompt. Otherwise, the model may generate large amounts whitespace.\nExamples # Generate request (Streaming) # Request # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34; }\u0026#39; Copy Response # A stream of JSON objects is returned:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T08:52:19.385406455-07:00\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;The\u0026#34;, \u0026#34;done\u0026#34;: false } Copy The final response in the stream also includes additional data about the generation:\ntotal_duration: time spent generating the response load_duration: time spent in nanoseconds loading the model prompt_eval_count: number of tokens in the prompt prompt_eval_duration: time spent in nanoseconds evaluating the prompt eval_count: number of tokens in the response eval_duration: time in nanoseconds spent generating the response context: an encoding of the conversation used in this response, this can be sent in the next request to keep a conversational memory response: empty if the response was streamed, if not streamed, this will contain the full response To calculate how fast the response is generated in tokens per second (token/s), divide eval_count / eval_duration.\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T19:22:45.499127Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;context\u0026#34;: [1, 2, 3], \u0026#34;total_duration\u0026#34;: 10706818083, \u0026#34;load_duration\u0026#34;: 6338219291, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 130079000, \u0026#34;eval_count\u0026#34;: 259, \u0026#34;eval_duration\u0026#34;: 4232710000 } Copy Request (No streaming) # Request # A response can be received in one reply when streaming is off.\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;stream\u0026#34;: false }\u0026#39; Copy Response # If stream is set to false, the response will be a single JSON object:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T19:22:45.499127Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;The sky is blue because it is the color of the sky.\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;context\u0026#34;: [1, 2, 3], \u0026#34;total_duration\u0026#34;: 5043500667, \u0026#34;load_duration\u0026#34;: 5025959, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 325953000, \u0026#34;eval_count\u0026#34;: 290, \u0026#34;eval_duration\u0026#34;: 4709213000 } Copy Request (JSON mode) # When format is set to json, the output will always be a well-formed JSON object. It\u0026rsquo;s important to also instruct the model to respond in JSON.\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;What color is the sky at different times of the day? Respond using JSON\u0026#34;, \u0026#34;format\u0026#34;: \u0026#34;json\u0026#34;, \u0026#34;stream\u0026#34;: false }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-11-09T21:07:55.186497Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;{\\n\\\u0026#34;morning\\\u0026#34;: {\\n\\\u0026#34;color\\\u0026#34;: \\\u0026#34;blue\\\u0026#34;\\n},\\n\\\u0026#34;noon\\\u0026#34;: {\\n\\\u0026#34;color\\\u0026#34;: \\\u0026#34;blue-gray\\\u0026#34;\\n},\\n\\\u0026#34;afternoon\\\u0026#34;: {\\n\\\u0026#34;color\\\u0026#34;: \\\u0026#34;warm gray\\\u0026#34;\\n},\\n\\\u0026#34;evening\\\u0026#34;: {\\n\\\u0026#34;color\\\u0026#34;: \\\u0026#34;orange\\\u0026#34;\\n}\\n}\\n\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;context\u0026#34;: [1, 2, 3], \u0026#34;total_duration\u0026#34;: 4648158584, \u0026#34;load_duration\u0026#34;: 4071084, \u0026#34;prompt_eval_count\u0026#34;: 36, \u0026#34;prompt_eval_duration\u0026#34;: 439038000, \u0026#34;eval_count\u0026#34;: 180, \u0026#34;eval_duration\u0026#34;: 4196918000 } Copy The value of response will be a string containing JSON similar to:\n{ \u0026#34;morning\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;blue\u0026#34; }, \u0026#34;noon\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;blue-gray\u0026#34; }, \u0026#34;afternoon\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;warm gray\u0026#34; }, \u0026#34;evening\u0026#34;: { \u0026#34;color\u0026#34;: \u0026#34;orange\u0026#34; } } Copy Request (with images) # To submit images to multimodal models such as llava or bakllava, provide a list of base64-encoded images:\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llava\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;What is in this picture?\u0026#34;, \u0026#34;stream\u0026#34;: false, \u0026#34;images\u0026#34;: [\u0026#34;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\u0026#34;] }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;llava\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-11-03T15:36:02.583064Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;A happy cartoon character, which is cute and cheerful.\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;context\u0026#34;: [1, 2, 3], \u0026#34;total_duration\u0026#34;: 2938432250, \u0026#34;load_duration\u0026#34;: 2559292, \u0026#34;prompt_eval_count\u0026#34;: 1, \u0026#34;prompt_eval_duration\u0026#34;: 2195557000, \u0026#34;eval_count\u0026#34;: 44, \u0026#34;eval_duration\u0026#34;: 736432000 } Copy Request (Raw Mode) # In some cases, you may wish to bypass the templating system and provide a full prompt. In this case, you can use the raw parameter to disable templating. Also note that raw mode will not return a context.\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;[INST] why is the sky blue? [/INST]\u0026#34;, \u0026#34;raw\u0026#34;: true, \u0026#34;stream\u0026#34;: false }\u0026#39; Copy Request (Reproducible outputs) # For reproducible outputs, set temperature to 0 and seed to a number:\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;seed\u0026#34;: 123, \u0026#34;temperature\u0026#34;: 0 } }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-11-03T15:36:02.583064Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34; The sky appears blue because of a phenomenon called Rayleigh scattering.\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 8493852375, \u0026#34;load_duration\u0026#34;: 6589624375, \u0026#34;prompt_eval_count\u0026#34;: 14, \u0026#34;prompt_eval_duration\u0026#34;: 119039000, \u0026#34;eval_count\u0026#34;: 110, \u0026#34;eval_duration\u0026#34;: 1779061000 } Copy Generate request (With options) # If you want to set custom options for the model at runtime rather than in the Modelfile, you can do so with the options parameter. This example sets every available option, but you can set any of them individually and omit the ones you do not want to override.\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;stream\u0026#34;: false, \u0026#34;options\u0026#34;: { \u0026#34;num_keep\u0026#34;: 5, \u0026#34;seed\u0026#34;: 42, \u0026#34;num_predict\u0026#34;: 100, \u0026#34;top_k\u0026#34;: 20, \u0026#34;top_p\u0026#34;: 0.9, \u0026#34;tfs_z\u0026#34;: 0.5, \u0026#34;typical_p\u0026#34;: 0.7, \u0026#34;repeat_last_n\u0026#34;: 33, \u0026#34;temperature\u0026#34;: 0.8, \u0026#34;repeat_penalty\u0026#34;: 1.2, \u0026#34;presence_penalty\u0026#34;: 1.5, \u0026#34;frequency_penalty\u0026#34;: 1.0, \u0026#34;mirostat\u0026#34;: 1, \u0026#34;mirostat_tau\u0026#34;: 0.8, \u0026#34;mirostat_eta\u0026#34;: 0.6, \u0026#34;penalize_newline\u0026#34;: true, \u0026#34;stop\u0026#34;: [\u0026#34;\\n\u0026#34;, \u0026#34;user:\u0026#34;], \u0026#34;numa\u0026#34;: false, \u0026#34;num_ctx\u0026#34;: 1024, \u0026#34;num_batch\u0026#34;: 2, \u0026#34;num_gqa\u0026#34;: 1, \u0026#34;num_gpu\u0026#34;: 1, \u0026#34;main_gpu\u0026#34;: 0, \u0026#34;low_vram\u0026#34;: false, \u0026#34;f16_kv\u0026#34;: true, \u0026#34;vocab_only\u0026#34;: false, \u0026#34;use_mmap\u0026#34;: true, \u0026#34;use_mlock\u0026#34;: false, \u0026#34;rope_frequency_base\u0026#34;: 1.1, \u0026#34;rope_frequency_scale\u0026#34;: 0.8, \u0026#34;num_thread\u0026#34;: 8 } }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T19:22:45.499127Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;The sky is blue because it is the color of the sky.\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;context\u0026#34;: [1, 2, 3], \u0026#34;total_duration\u0026#34;: 4935886791, \u0026#34;load_duration\u0026#34;: 534986708, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 107345000, \u0026#34;eval_count\u0026#34;: 237, \u0026#34;eval_duration\u0026#34;: 4289432000 } Copy Load a model # If an empty prompt is provided, the model will be loaded into memory.\nRequest # curl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34; }\u0026#39; Copy Response # A single JSON object is returned:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-12-18T19:52:07.071755Z\u0026#34;, \u0026#34;response\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;done\u0026#34;: true } Copy Generate a chat completion # POST /api/chat Copy Generate the next message in a chat with a provided model. This is a streaming endpoint, so there will be a series of responses. Streaming can be disabled using \u0026quot;stream\u0026quot;: false. The final response object will include statistics and additional data from the request.\nParameters # model: (required) the model name messages: the messages of the chat, this can be used to keep a chat memory The message object has the following fields:\nrole: the role of the message, either system, user or assistant content: the content of the message images (optional): a list of images to include in the message (for multimodal models such as llava) Advanced parameters (optional):\nformat: the format to return a response in. Currently the only accepted value is json options: additional model parameters listed in the documentation for the Modelfile such as temperature stream: if false the response will be returned as a single response object, rather than a stream of objects keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m) Examples # Chat Request (Streaming) # Request # Send a chat message with a streaming response.\ncurl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;why is the sky blue?\u0026#34; } ] }\u0026#39; Copy Response # A stream of JSON objects is returned:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T08:52:19.385406455-07:00\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;The\u0026#34;, \u0026#34;images\u0026#34;: null }, \u0026#34;done\u0026#34;: false } Copy Final response:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T19:22:45.499127Z\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 4883583458, \u0026#34;load_duration\u0026#34;: 1334875, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 342546000, \u0026#34;eval_count\u0026#34;: 282, \u0026#34;eval_duration\u0026#34;: 4535599000 } Copy Chat request (No streaming) # Request # curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;why is the sky blue?\u0026#34; } ], \u0026#34;stream\u0026#34;: false }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;registry.ollama.ai/library/llama3:latest\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-12-12T14:13:43.416799Z\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello! How are you today?\u0026#34; }, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 5191566416, \u0026#34;load_duration\u0026#34;: 2154458, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 383809000, \u0026#34;eval_count\u0026#34;: 298, \u0026#34;eval_duration\u0026#34;: 4799921000 } Copy Chat request (With History) # Send a chat message with a conversation history. You can use this same approach to start the conversation using multi-shot or chain-of-thought prompting.\nRequest # curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;why is the sky blue?\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;due to rayleigh scattering.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;how is that different than mie scattering?\u0026#34; } ] }\u0026#39; Copy Response # A stream of JSON objects is returned:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T08:52:19.385406455-07:00\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;The\u0026#34; }, \u0026#34;done\u0026#34;: false } Copy Final response:\n{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-08-04T19:22:45.499127Z\u0026#34;, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 8113331500, \u0026#34;load_duration\u0026#34;: 6396458, \u0026#34;prompt_eval_count\u0026#34;: 61, \u0026#34;prompt_eval_duration\u0026#34;: 398801000, \u0026#34;eval_count\u0026#34;: 468, \u0026#34;eval_duration\u0026#34;: 7701267000 } Copy Chat request (with images) # Request # Send a chat message with a conversation history.\ncurl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llava\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;what is in this image?\u0026#34;, \u0026#34;images\u0026#34;: [\u0026#34;iVBORw0KGgoAAAANSUhEUgAAAG0AAABmCAYAAADBPx+VAAAACXBIWXMAAAsTAAALEwEAmpwYAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAA3VSURBVHgB7Z27r0zdG8fX743i1bi1ikMoFMQloXRpKFFIqI7LH4BEQ+NWIkjQuSWCRIEoULk0gsK1kCBI0IhrQVT7tz/7zZo888yz1r7MnDl7z5xvsjkzs2fP3uu71nNfa7lkAsm7d++Sffv2JbNmzUqcc8m0adOSzZs3Z+/XES4ZckAWJEGWPiCxjsQNLWmQsWjRIpMseaxcuTKpG/7HP27I8P79e7dq1ars/yL4/v27S0ejqwv+cUOGEGGpKHR37tzJCEpHV9tnT58+dXXCJDdECBE2Ojrqjh071hpNECjx4cMHVycM1Uhbv359B2F79+51586daxN/+pyRkRFXKyRDAqxEp4yMlDDzXG1NPnnyJKkThoK0VFd1ELZu3TrzXKxKfW7dMBQ6bcuWLW2v0VlHjx41z717927ba22U9APcw7Nnz1oGEPeL3m3p2mTAYYnFmMOMXybPPXv2bNIPpFZr1NHn4HMw0KRBjg9NuRw95s8PEcz/6DZELQd/09C9QGq5RsmSRybqkwHGjh07OsJSsYYm3ijPpyHzoiacg35MLdDSIS/O1yM778jOTwYUkKNHWUzUWaOsylE00MyI0fcnOwIdjvtNdW/HZwNLGg+sR1kMepSNJXmIwxBZiG8tDTpEZzKg0GItNsosY8USkxDhD0Rinuiko2gfL/RbiD2LZAjU9zKQJj8RDR0vJBR1/Phx9+PHj9Z7REF4nTZkxzX4LCXHrV271qXkBAPGfP/atWvu/PnzHe4C97F48eIsRLZ9+3a3f/9+87dwP1JxaF7/3r17ba+5l4EcaVo0lj3SBq5kGTJSQmLWMjgYNei2GPT1MuMqGTDEFHzeQSP2wi/jGnkmPJ/nhccs44jvDAxpVcxnq0F6eT8h4ni/iIWpR5lPyA6ETkNXoSukvpJAD3AsXLiwpZs49+fPn5ke4j10TqYvegSfn0OnafC+Tv9ooA/JPkgQysqQNBzagXY55nO/oa1F7qvIPWkRL12WRpMWUvpVDYmxAPehxWSe8ZEXL20sadYIozfmNch4QJPAfeJgW3rNsnzphBKNJM2KKODo1rVOMRYik5ETy3ix4qWNI81qAAirizgMIc+yhTytx0JWZuNI03qsrgWlGtwjoS9XwgUhWGyhUaRZZQNNIEwCiXD16tXcAHUs79co0vSD8rrJCIW98pzvxpAWyyo3HYwqS0+H0BjStClcZJT5coMm6D2LOF8TolGJtK9fvyZpyiC5ePFi9nc/oJU4eiEP0jVoAnHa9wyJycITMP78+eMeP37sXrx44d6+fdt6f82aNdkx1pg9e3Zb5W+RSRE+n+VjksQWifvVaTKFhn5O8my63K8Qabdv33b379/PiAP//vuvW7BggZszZ072/+TJk91YgkafPn166zXB1rQHFvouAWHq9z3SEevSUerqCn2/dDCeta2jxYbr69evk4MHDyY7d+7MjhMnTiTPnz9Pfv/+nfQT2ggpO2dMF8cghuoM7Ygj5iWCqRlGFml0QC/ftGmTmzt3rmsaKDsgBSPh0/8yPeLLBihLkOKJc0jp8H8vUzcxIA1k6QJ/c78tWEyj5P3o4u9+jywNPdJi5rAH9x0KHcl4Hg570eQp3+vHXGyrmEeigzQsQsjavXt38ujRo44LQuDDhw+TW7duRS1HGgMxhNXHgflaNTOsHyKvHK5Ijo2jbFjJBQK9YwFd6RVMzfgRBmEfP37suBBm/p49e1qjEP2mwTViNRo0VJWH1deMXcNK08uUjVUu7s/zRaL+oLNxz1bpANco4npUgX4G2eFbpDFyQoQxojBCpEGSytmOH8qrH5Q9vuzD6ofQylkCUmh8DBAr+q8JCyVNtWQIidKQE9wNtLSQnS4jDSsxNHogzFuQBw4cyM61UKVsjfr3ooBkPSqqQHesUPWVtzi9/vQi1T+rJj7WiTz4Pt/l3LxUkr5P2VYZaZ4URpsE+st/dujQoaBBYokbrz/8TJNQYLSonrPS9kUaSkPeZyj1AWSj+d+VBoy1pIWVNed8P0Ll/ee5HdGRhrHhR5GGN0r4LGZBaj8oFDJitBTJzIZgFcmU0Y8ytWMZMzJOaXUSrUs5RxKnrxmbb5YXO9VGUhtpXldhEUogFr3IzIsvlpmdosVcGVGXFWp2oU9kLFL3dEkSz6NHEY1sjSRdIuDFWEhd8KxFqsRi1uM/nz9/zpxnwlESONdg6dKlbsaMGS4EHFHtjFIDHwKOo46l4TxSuxgDzi+rE2jg+BaFruOX4HXa0Nnf1lwAPufZeF8/r6zD97WK2qFnGjBxTw5qNGPxT+5T/r7/7RawFC3j4vTp09koCxkeHjqbHJqArmH5UrFKKksnxrK7FuRIs8STfBZv+luugXZ2pR/pP9Ois4z+TiMzUUkUjD0iEi1fzX8GmXyuxUBRcaUfykV0YZnlJGKQpOiGB76x5GeWkWWJc3mOrK6S7xdND+W5N6XyaRgtWJFe13GkaZnKOsYqGdOVVVbGupsyA/l7emTLHi7vwTdirNEt0qxnzAvBFcnQF16xh/TMpUuXHDowhlA9vQVraQhkudRdzOnK+04ZSP3DUhVSP61YsaLtd/ks7ZgtPcXqPqEafHkdqa84X6aCeL7YWlv6edGFHb+ZFICPlljHhg0bKuk0CSvVznWsotRu433alNdFrqG45ejoaPCaUkWERpLXjzFL2Rpllp7PJU2a/v7Ab8N05/9t27Z16KUqoFGsxnI9EosS2niSYg9SpU6B4JgTrvVW1flt1sT+0ADIJU2maXzcUTraGCRaL1Wp9rUMk16PMom8QhruxzvZIegJjFU7LLCePfS8uaQdPny4jTTL0dbee5mYokQsXTIWNY46kuMbnt8Kmec+LGWtOVIl9cT1rCB0V8WqkjAsRwta93TbwNYoGKsUSChN44lgBNCoHLHzquYKrU6qZ8lolCIN0Rh6cP0Q3U6I6IXILYOQI513hJaSKAorFpuHXJNfVlpRtmYBk1Su1obZr5dnKAO+L10Hrj3WZW+E3qh6IszE37F6EB+68mGpvKm4eb9bFrlzrok7fvr0Kfv727dvWRmdVTJHw0qiiCUSZ6wCK+7XL/AcsgNyL74DQQ730sv78Su7+t/A36MdY0sW5o40ahslXr58aZ5HtZB8GH64m9EmMZ7FpYw4T6QnrZfgenrhFxaSiSGXtPnz57e9TkNZLvTjeqhr734CNtrK41L40sUQckmj1lGKQ0rC37x544r8eNXRpnVE3ZZY7zXo8NomiO0ZUCj2uHz58rbXoZ6gc0uA+F6ZeKS/jhRDUq8MKrTho9fEkihMmhxtBI1DxKFY9XLpVcSkfoi8JGnToZO5sU5aiDQIW716ddt7ZLYtMQlhECdBGXZZMWldY5BHm5xgAroWj4C0hbYkSc/jBmggIrXJWlZM6pSETsEPGqZOndr2uuuR5rF169a2HoHPdurUKZM4CO1WTPqaDaAd+GFGKdIQkxAn9RuEWcTRyN2KSUgiSgF5aWzPTeA/lN5rZubMmR2bE4SIC4nJoltgAV/dVefZm72AtctUCJU2CMJ327hxY9t7EHbkyJFseq+EJSY16RPo3Dkq1kkr7+q0bNmyDuLQcZBEPYmHVdOBiJyIlrRDq41YPWfXOxUysi5fvtyaj+2BpcnsUV/oSoEMOk2CQGlr4ckhBwaetBhjCwH0ZHtJROPJkyc7UjcYLDjmrH7ADTEBXFfOYmB0k9oYBOjJ8b4aOYSe7QkKcYhFlq3QYLQhSidNmtS2RATwy8YOM3EQJsUjKiaWZ+vZToUQgzhkHXudb/PW5YMHD9yZM2faPsMwoc7RciYJXbGuBqJ1UIGKKLv915jsvgtJxCZDubdXr165mzdvtr1Hz5LONA8jrUwKPqsmVesKa49S3Q4WxmRPUEYdTjgiUcfUwLx589ySJUva3oMkP6IYddq6HMS4o55xBJBUeRjzfa4Zdeg56QZ43LhxoyPo7Lf1kNt7oO8wWAbNwaYjIv5lhyS7kRf96dvm5Jah8vfvX3flyhX35cuX6HfzFHOToS1H4BenCaHvO8pr8iDuwoUL7tevX+b5ZdbBair0xkFIlFDlW4ZknEClsp/TzXyAKVOmmHWFVSbDNw1l1+4f90U6IY/q4V27dpnE9bJ+v87QEydjqx/UamVVPRG+mwkNTYN+9tjkwzEx+atCm/X9WvWtDtAb68Wy9LXa1UmvCDDIpPkyOQ5ZwSzJ4jMrvFcr0rSjOUh+GcT4LSg5ugkW1Io0/SCDQBojh0hPlaJdah+tkVYrnTZowP8iq1F1TgMBBauufyB33x1v+NWFYmT5KmppgHC+NkAgbmRkpD3yn9QIseXymoTQFGQmIOKTxiZIWpvAatenVqRVXf2nTrAWMsPnKrMZHz6bJq5jvce6QK8J1cQNgKxlJapMPdZSR64/UivS9NztpkVEdKcrs5alhhWP9NeqlfWopzhZScI6QxseegZRGeg5a8C3Re1Mfl1ScP36ddcUaMuv24iOJtz7sbUjTS4qBvKmstYJoUauiuD3k5qhyr7QdUHMeCgLa1Ear9NquemdXgmum4fvJ6w1lqsuDhNrg1qSpleJK7K3TF0Q2jSd94uSZ60kK1e3qyVpQK6PVWXp2/FC3mp6jBhKKOiY2h3gtUV64TWM6wDETRPLDfSakXmH3w8g9Jlug8ZtTt4kVF0kLUYYmCCtD/DrQ5YhMGbA9L3ucdjh0y8kOHW5gU/VEEmJTcL4Pz/f7mgoAbYkAAAAAElFTkSuQmCC\u0026#34;] } ] }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;llava\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-12-13T22:42:50.203334Z\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34; The image features a cute, little pig with an angry facial expression. It\u0026#39;s wearing a heart on its shirt and is waving in the air. This scene appears to be part of a drawing or sketching project.\u0026#34;, \u0026#34;images\u0026#34;: null }, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 1668506709, \u0026#34;load_duration\u0026#34;: 1986209, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 359682000, \u0026#34;eval_count\u0026#34;: 83, \u0026#34;eval_duration\u0026#34;: 1303285000 } Copy Chat request (Reproducible outputs) # Request # curl http://localhost:11434/api/chat -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34; } ], \u0026#34;options\u0026#34;: { \u0026#34;seed\u0026#34;: 101, \u0026#34;temperature\u0026#34;: 0 } }\u0026#39; Copy Response # { \u0026#34;model\u0026#34;: \u0026#34;registry.ollama.ai/library/llama3:latest\u0026#34;, \u0026#34;created_at\u0026#34;: \u0026#34;2023-12-12T14:13:43.416799Z\u0026#34;, \u0026#34;message\u0026#34;: { \u0026#34;role\u0026#34;: \u0026#34;assistant\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello! How are you today?\u0026#34; }, \u0026#34;done\u0026#34;: true, \u0026#34;total_duration\u0026#34;: 5191566416, \u0026#34;load_duration\u0026#34;: 2154458, \u0026#34;prompt_eval_count\u0026#34;: 26, \u0026#34;prompt_eval_duration\u0026#34;: 383809000, \u0026#34;eval_count\u0026#34;: 298, \u0026#34;eval_duration\u0026#34;: 4799921000 } Copy Create a Model # POST /api/create Copy Create a model from a Modelfile. It is recommended to set modelfile to the content of the Modelfile rather than just set path. This is a requirement for remote create. Remote model creation must also create any file blobs, fields such as FROM and ADAPTER, explicitly with the server using Create a Blob and the value to the path indicated in the response.\nParameters # name: name of the model to create modelfile (optional): contents of the Modelfile stream: (optional) if false the response will be returned as a single response object, rather than a stream of objects path (optional): path to the Modelfile Examples # Create a new model # Create a new model from a Modelfile.\nRequest # curl http://localhost:11434/api/create -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;mario\u0026#34;, \u0026#34;modelfile\u0026#34;: \u0026#34;FROM llama3\\nSYSTEM You are mario from Super Mario Bros.\u0026#34; }\u0026#39; Copy Response # A stream of JSON objects. Notice that the final JSON object shows a \u0026quot;status\u0026quot;: \u0026quot;success\u0026quot;.\n{\u0026#34;status\u0026#34;:\u0026#34;reading model metadata\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;creating system layer\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;using already created layer sha256:22f7f8ef5f4c791c1b03d7eb414399294764d7cc82c7e94aa81a1feb80a983a2\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;using already created layer sha256:8c17c2ebb0ea011be9981cc3922db8ca8fa61e828c5d3f44cb6ae342bf80460b\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;using already created layer sha256:7c23fb36d80141c4ab8cdbb61ee4790102ebd2bf7aeff414453177d4f2110e5d\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;using already created layer sha256:2e0493f67d0c8c9c68a8aeacdf6a38a2151cb3c4c1d42accf296e19810527988\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;using already created layer sha256:2759286baa875dc22de5394b4a925701b1896a7e3f8e53275c36f75a877a82c9\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;writing layer sha256:df30045fe90f0d750db82a058109cecd6d4de9c90a3d75b19c09e5f64580bb42\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;writing layer sha256:f18a68eb09bf925bb1b669490407c1b1251c5db98dc4d3d81f3088498ea55690\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;writing manifest\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;} Copy Check if a Blob Exists # HEAD /api/blobs/:digest Copy Ensures that the file blob used for a FROM or ADAPTER field exists on the server. This is checking your Ollama server and not Ollama.ai.\nQuery Parameters # digest: the SHA256 digest of the blob Examples # Request # curl -I http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2 Copy Response # Return 200 OK if the blob exists, 404 Not Found if it does not.\nCreate a Blob # POST /api/blobs/:digest Copy Create a blob from a file on the server. Returns the server file path.\nQuery Parameters # digest: the expected SHA256 digest of the file Examples # Request # curl -T model.bin -X POST http://localhost:11434/api/blobs/sha256:29fdb92e57cf0827ded04ae6461b5931d01fa595843f55d36f5b275a52087dd2 Copy Response # Return 201 Created if the blob was successfully created, 400 Bad Request if the digest used is not expected.\nList Local Models # GET /api/tags Copy List models that are available locally.\nExamples # Request # curl http://localhost:11434/api/tags Copy Response # A single JSON object will be returned.\n{ \u0026#34;models\u0026#34;: [ { \u0026#34;name\u0026#34;: \u0026#34;codellama:13b\u0026#34;, \u0026#34;modified_at\u0026#34;: \u0026#34;2023-11-04T14:56:49.277302595-07:00\u0026#34;, \u0026#34;size\u0026#34;: 7365960935, \u0026#34;digest\u0026#34;: \u0026#34;9f438cb9cd581fc025612d27f7c1a6669ff83a8bb0ed86c94fcf4c5440555697\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;gguf\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;llama\u0026#34;, \u0026#34;families\u0026#34;: null, \u0026#34;parameter_size\u0026#34;: \u0026#34;13B\u0026#34;, \u0026#34;quantization_level\u0026#34;: \u0026#34;Q4_0\u0026#34; } }, { \u0026#34;name\u0026#34;: \u0026#34;llama3:latest\u0026#34;, \u0026#34;modified_at\u0026#34;: \u0026#34;2023-12-07T09:32:18.757212583-08:00\u0026#34;, \u0026#34;size\u0026#34;: 3825819519, \u0026#34;digest\u0026#34;: \u0026#34;fe938a131f40e6f6d40083c9f0f430a515233eb2edaa6d72eb85c50d64f2300e\u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;gguf\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;llama\u0026#34;, \u0026#34;families\u0026#34;: null, \u0026#34;parameter_size\u0026#34;: \u0026#34;7B\u0026#34;, \u0026#34;quantization_level\u0026#34;: \u0026#34;Q4_0\u0026#34; } } ] } Copy Show Model Information # POST /api/show Copy Show information about a model including details, modelfile, template, parameters, license, and system prompt.\nParameters # name: name of the model to show Examples # Request # curl http://localhost:11434/api/show -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;llama3\u0026#34; }\u0026#39; Copy Response # { \u0026#34;modelfile\u0026#34;: \u0026#34;# Modelfile generated by \\\u0026#34;ollama show\\\u0026#34;\\n# To build a new Modelfile based on this one, replace the FROM line with:\\n# FROM llava:latest\\n\\nFROM /Users/matt/.ollama/models/blobs/sha256:200765e1283640ffbd013184bf496e261032fa75b99498a9613be4e94d63ad52\\nTEMPLATE \\\u0026#34;\\\u0026#34;\\\u0026#34;{{ .System }}\\nUSER: {{ .Prompt }}\\nASSSISTANT: \\\u0026#34;\\\u0026#34;\\\u0026#34;\\nPARAMETER num_ctx 4096\\nPARAMETER stop \\\u0026#34;\\u003c/s\\u003e\\\u0026#34;\\nPARAMETER stop \\\u0026#34;USER:\\\u0026#34;\\nPARAMETER stop \\\u0026#34;ASSSISTANT:\\\u0026#34;\u0026#34;, \u0026#34;parameters\u0026#34;: \u0026#34;num_ctx 4096\\nstop \\u003c/s\\u003e\\nstop USER:\\nstop ASSSISTANT:\u0026#34;, \u0026#34;template\u0026#34;: \u0026#34;{{ .System }}\\nUSER: {{ .Prompt }}\\nASSSISTANT: \u0026#34;, \u0026#34;details\u0026#34;: { \u0026#34;format\u0026#34;: \u0026#34;gguf\u0026#34;, \u0026#34;family\u0026#34;: \u0026#34;llama\u0026#34;, \u0026#34;families\u0026#34;: [\u0026#34;llama\u0026#34;, \u0026#34;clip\u0026#34;], \u0026#34;parameter_size\u0026#34;: \u0026#34;7B\u0026#34;, \u0026#34;quantization_level\u0026#34;: \u0026#34;Q4_0\u0026#34; } } Copy Copy a Model # POST /api/copy Copy Copy a model. Creates a model with another name from an existing model.\nExamples # Request # curl http://localhost:11434/api/copy -d \u0026#39;{ \u0026#34;source\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;destination\u0026#34;: \u0026#34;llama3-backup\u0026#34; }\u0026#39; Copy Response # Returns a 200 OK if successful, or a 404 Not Found if the source model doesn\u0026rsquo;t exist.\nDelete a Model # DELETE /api/delete Copy Delete a model and its data.\nParameters # name: model name to delete Examples # Request # curl -X DELETE http://localhost:11434/api/delete -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;llama3:13b\u0026#34; }\u0026#39; Copy Response # Returns a 200 OK if successful, 404 Not Found if the model to be deleted doesn\u0026rsquo;t exist.\nPull a Model # POST /api/pull Copy Download a model from the ollama library. Cancelled pulls are resumed from where they left off, and multiple calls will share the same download progress.\nParameters # name: name of the model to pull insecure: (optional) allow insecure connections to the library. Only use this if you are pulling from your own library during development. stream: (optional) if false the response will be returned as a single response object, rather than a stream of objects Examples # Request # curl http://localhost:11434/api/pull -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;llama3\u0026#34; }\u0026#39; Copy Response # If stream is not specified, or set to true, a stream of JSON objects is returned:\nThe first object is the manifest:\n{ \u0026#34;status\u0026#34;: \u0026#34;pulling manifest\u0026#34; } Copy Then there is a series of downloading responses. Until any of the download is completed, the completed key may not be included. The number of files to be downloaded depends on the number of layers specified in the manifest.\n{ \u0026#34;status\u0026#34;: \u0026#34;downloading digestname\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;digestname\u0026#34;, \u0026#34;total\u0026#34;: 2142590208, \u0026#34;completed\u0026#34;: 241970 } Copy After all the files are downloaded, the final responses are:\n{ \u0026#34;status\u0026#34;: \u0026#34;verifying sha256 digest\u0026#34; } { \u0026#34;status\u0026#34;: \u0026#34;writing manifest\u0026#34; } { \u0026#34;status\u0026#34;: \u0026#34;removing any unused layers\u0026#34; } { \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Copy if stream is set to false, then the response is a single JSON object:\n{ \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Copy Push a Model # POST /api/push Copy Upload a model to a model library. Requires registering for ollama.ai and adding a public key first.\nParameters # name: name of the model to push in the form of \u0026lt;namespace\u0026gt;/\u0026lt;model\u0026gt;:\u0026lt;tag\u0026gt; insecure: (optional) allow insecure connections to the library. Only use this if you are pushing to your library during development. stream: (optional) if false the response will be returned as a single response object, rather than a stream of objects Examples # Request # curl http://localhost:11434/api/push -d \u0026#39;{ \u0026#34;name\u0026#34;: \u0026#34;mattw/pygmalion:latest\u0026#34; }\u0026#39; Copy Response # If stream is not specified, or set to true, a stream of JSON objects is returned:\n{ \u0026#34;status\u0026#34;: \u0026#34;retrieving manifest\u0026#34; } Copy and then:\n{ \u0026#34;status\u0026#34;: \u0026#34;starting upload\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\u0026#34;, \u0026#34;total\u0026#34;: 1928429856 } Copy Then there is a series of uploading responses:\n{ \u0026#34;status\u0026#34;: \u0026#34;starting upload\u0026#34;, \u0026#34;digest\u0026#34;: \u0026#34;sha256:bc07c81de745696fdf5afca05e065818a8149fb0c77266fb584d9b2cba3711ab\u0026#34;, \u0026#34;total\u0026#34;: 1928429856 } Copy Finally, when the upload is complete:\n{\u0026#34;status\u0026#34;:\u0026#34;pushing manifest\u0026#34;} {\u0026#34;status\u0026#34;:\u0026#34;success\u0026#34;} Copy If stream is set to false, then the response is a single JSON object:\n{ \u0026#34;status\u0026#34;: \u0026#34;success\u0026#34; } Copy Generate Embeddings # POST /api/embeddings Copy Generate embeddings from a model\nParameters # model: name of model to generate embeddings from prompt: text to generate embeddings for Advanced parameters:\noptions: additional model parameters listed in the documentation for the Modelfile such as temperature keep_alive: controls how long the model will stay loaded into memory following the request (default: 5m) Examples # Request # curl http://localhost:11434/api/embeddings -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;all-minilm\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Here is an article about llamas...\u0026#34; }\u0026#39; Copy Response # { \u0026#34;embedding\u0026#34;: [ 0.5670403838157654, 0.009260174818336964, 0.23178744316101074, -0.2916173040866852, -0.8924556970596313, 0.8785552978515625, -0.34576427936553955, 0.5742510557174683, -0.04222835972905159, -0.137906014919281 ] } Copy ",
    "tags": []
},{
    "id": "c51f1b783d6e6b3bc97ce5906e9605d3",
    "title": "Development",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/development/",
    "relURI": "/development/",
    "body": " Development # Install required tools:\ncmake version 3.24 or higher go version 1.22 or higher gcc version 11.4.0 or higher brew install go cmake gcc Copy Optionally enable debugging and more verbose logging:\n# At build time export CGO_CFLAGS=\u0026#34;-g\u0026#34; # At runtime export OLLAMA_DEBUG=1 Copy Get the required libraries and build the native LLM code:\ngo generate ./... Copy Then build ollama:\ngo build . Copy Now you can run ollama:\n./ollama Copy Linux # Linux CUDA (NVIDIA) # Your operating system distribution may already have packages for NVIDIA CUDA. Distro packages are often preferable, but instructions are distro-specific. Please consult distro-specific docs for dependencies if available!\nInstall cmake and golang as well as NVIDIA CUDA development and runtime packages.\nTypically the build scripts will auto-detect CUDA, however, if your Linux distro or installation approach uses unusual paths, you can specify the location by specifying an environment variable CUDA_LIB_DIR to the location of the shared libraries, and CUDACXX to the location of the nvcc compiler. You can customize a set of target CUDA architectures by setting CMAKE_CUDA_ARCHITECTURES (e.g. \u0026ldquo;50;60;70\u0026rdquo;)\nThen generate dependencies:\ngo generate ./... Copy Then build the binary:\ngo build . Copy Linux ROCm (AMD) # Your operating system distribution may already have packages for AMD ROCm and CLBlast. Distro packages are often preferable, but instructions are distro-specific. Please consult distro-specific docs for dependencies if available!\nInstall CLBlast and ROCm development packages first, as well as cmake and golang.\nTypically the build scripts will auto-detect ROCm, however, if your Linux distro or installation approach uses unusual paths, you can specify the location by specifying an environment variable ROCM_PATH to the location of the ROCm install (typically /opt/rocm), and CLBlast_DIR to the location of the CLBlast install (typically /usr/lib/cmake/CLBlast). You can also customize the AMD GPU targets by setting AMDGPU_TARGETS (e.g. AMDGPU_TARGETS=\u0026quot;gfx1101;gfx1102\u0026quot;)\ngo generate ./... Copy Then build the binary:\ngo build . Copy ROCm requires elevated privileges to access the GPU at runtime. On most distros you can add your user account to the render group, or run as root.\nAdvanced CPU Settings # By default, running go generate ./... will compile a few different variations of the LLM library based on common CPU families and vector math capabilities, including a lowest-common-denominator which should run on almost any 64 bit CPU somewhat slowly. At runtime, Ollama will auto-detect the optimal variation to load. If you would like to build a CPU-based build customized for your processor, you can set OLLAMA_CUSTOM_CPU_DEFS to the llama.cpp flags you would like to use. For example, to compile an optimized binary for an Intel i9-9880H, you might use:\nOLLAMA_CUSTOM_CPU_DEFS=\u0026#34;-DLLAMA_AVX=on -DLLAMA_AVX2=on -DLLAMA_F16C=on -DLLAMA_FMA=on\u0026#34; go generate ./... go build . Copy Containerized Linux Build # If you have Docker available, you can build linux binaries with ./scripts/build_linux.sh which has the CUDA and ROCm dependencies included. The resulting binary is placed in ./dist\nWindows # Note: The windows build for Ollama is still under development.\nInstall required tools:\nMSVC toolchain - C/C++ and cmake as minimal requirements Go version 1.22 or higher MinGW (pick one variant) with GCC. MinGW-w64 MSYS2 $env:CGO_ENABLED=\u0026#34;1\u0026#34; go generate ./... go build . Copy Windows CUDA (NVIDIA) # In addition to the common Windows development tools described above, install CUDA after installing MSVC.\nNVIDIA CUDA Windows ROCm (AMD Radeon) # In addition to the common Windows development tools described above, install AMDs HIP package after installing MSVC.\nAMD HIP Strawberry Perl Lastly, add ninja.exe included with MSVC to the system path (e.g. C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\Common7\\IDE\\CommonExtensions\\Microsoft\\CMake\\Ninja).\n",
    "tags": []
},{
    "id": "4a1318da06786ac52626178ca960109e",
    "title": "Faq",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/faq/",
    "relURI": "/faq/",
    "body": " FAQ # How can I upgrade Ollama? # Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \u0026ldquo;Restart to update\u0026rdquo; to apply the update. Updates can also be installed by downloading the latest version manually.\nOn Linux, re-run the install script:\ncurl -fsSL https://ollama.com/install.sh | sh Copy How can I view the logs? # Review the Troubleshooting docs for more about using logs.\nIs my GPU compatible with Ollama? # Please refer to the GPU docs.\nHow can I specify the context window size? # By default, Ollama uses a context window size of 2048 tokens.\nTo change this when using ollama run, use /set parameter:\n/set parameter num_ctx 4096 Copy When using the API, specify the num_ctx parameter:\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;num_ctx\u0026#34;: 4096 } }\u0026#39; Copy How do I configure Ollama server? # Ollama server can be configured with environment variables.\nSetting environment variables on Mac # If Ollama is run as a macOS application, environment variables should be set using launchctl:\nFor each environment variable, call launchctl setenv.\nlaunchctl setenv OLLAMA_HOST \u0026#34;0.0.0.0\u0026#34; Copy Restart Ollama application.\nSetting environment variables on Linux # If Ollama is run as a systemd service, environment variables should be set using systemctl:\nEdit the systemd service by calling systemctl edit ollama.service. This will open an editor.\nFor each environment variable, add a line Environment under section [Service]:\n[Service] Environment=\u0026#34;OLLAMA_HOST=0.0.0.0\u0026#34; Copy Save and exit.\nReload systemd and restart Ollama:\nsystemctl daemon-reload systemctl restart ollama Copy Setting environment variables on Windows # On windows, Ollama inherits your user and system environment variables.\nFirst Quit Ollama by clicking on it in the task bar\nEdit system environment variables from the control panel\nEdit or create New variable(s) for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.\nClick OK/Apply to save\nRun ollama from a new terminal window\nHow can I expose Ollama on my network? # Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the OLLAMA_HOST environment variable.\nRefer to the section above for how to set environment variables on your platform.\nHow can I use Ollama with a proxy server? # Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:\nserver { listen 80; server_name example.com; # Replace with your domain or IP location / { proxy_pass http://localhost:11434; proxy_set_header Host localhost:11434; } } Copy How can I use Ollama with ngrok? # Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:\nngrok http 11434 --host-header=\u0026#34;localhost:11434\u0026#34; Copy How can I use Ollama with Cloudflare Tunnel? # To use Ollama with Cloudflare Tunnel, use the --url and --http-host-header flags:\ncloudflared tunnel --url http://localhost:11434 --http-host-header=\u0026#34;localhost:11434\u0026#34; Copy How can I allow additional web origins to access Ollama? # Ollama allows cross-origin requests from 127.0.0.1 and 0.0.0.0 by default. Additional origins can be configured with OLLAMA_ORIGINS.\nRefer to the section above for how to set environment variables on your platform.\nWhere are models stored? # macOS: ~/.ollama/models Linux: /usr/share/ollama/.ollama/models Windows: C:\\Users\\\u0026lt;username\u0026gt;\\.ollama\\models How do I set them to a different location? # If a different directory needs to be used, set the environment variable OLLAMA_MODELS to the chosen directory.\nRefer to the section above for how to set environment variables on your platform.\nDoes Ollama send my prompts and answers back to ollama.com? # No. Ollama runs locally, and conversation data does not leave your machine.\nHow can I use Ollama in Visual Studio Code? # There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of extensions \u0026amp; plugins at the bottom of the main repository readme.\nHow do I use Ollama behind a proxy? # Ollama is compatible with proxy servers if HTTP_PROXY or HTTPS_PROXY are configured. When using either variables, ensure it is set where ollama serve can access the values. When using HTTPS_PROXY, ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.\nHow do I use Ollama behind a proxy in Docker? # The Ollama Docker container image can be configured to use a proxy by passing -e HTTPS_PROXY=https://proxy.example.com when starting the container.\nAlternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on macOS, Windows, and Linux, and Docker daemon with systemd.\nEnsure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.\nFROM ollama/ollama COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt RUN update-ca-certificates Copy Build and run this image:\ndocker build -t ollama-with-ca . docker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca Copy How do I use Ollama with GPU acceleration in Docker? # The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the nvidia-container-toolkit. See ollama/ollama for more details.\nGPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.\nWhy is networking slow in WSL2 on Windows 10? # This can impact both installing Ollama, as well as downloading models.\nOpen Control Panel \u0026gt; Networking and Internet \u0026gt; View network status and tasks and click on Change adapter settings on the left panel. Find the vEthernel (WSL) adapter, right click and select Properties. Click on Configure and open the Advanced tab. Search through each of the properties until you find Large Send Offload Version 2 (IPv4) and Large Send Offload Version 2 (IPv6). Disable both of these properties.\nHow can I pre-load a model to get faster response times? # If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the /api/generate and /api/chat API endpoints.\nTo preload the mistral model using the generate endpoint, use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;}\u0026#39; Copy To use the chat completions endpoint, use:\ncurl http://localhost:11434/api/chat -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;}\u0026#39; Copy How do I keep a model loaded in memory or make it unload immediately? # By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you are making numerous requests to the LLM. You may, however, want to free up the memory before the 5 minutes have elapsed or keep the model loaded indefinitely. Use the keep_alive parameter with either the /api/generate and /api/chat API endpoints to control how long the model is left in memory.\nThe keep_alive parameter can be set to:\na duration string (such as \u0026ldquo;10m\u0026rdquo; or \u0026ldquo;24h\u0026rdquo;) a number in seconds (such as 3600) any negative number which will keep the model loaded in memory (e.g. -1 or \u0026ldquo;-1m\u0026rdquo;) \u0026lsquo;0\u0026rsquo; which will unload the model immediately after generating a response For example, to preload a model and leave it in memory use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;keep_alive\u0026#34;: -1}\u0026#39; Copy To unload the model and free up memory use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;keep_alive\u0026#34;: 0}\u0026#39; Copy Alternatively, you can change the amount of time all models are loaded into memory by setting the OLLAMA_KEEP_ALIVE environment variable when starting the Ollama server. The OLLAMA_KEEP_ALIVE variable uses the same parameter types as the keep_alive parameter types mentioned above. Refer to section explaining how to configure the Ollama server to correctly set the environment variable.\nIf you wish to override the OLLAMA_KEEP_ALIVE setting, use the keep_alive API parameter with the /api/generate or /api/chat API endpoints.\n",
    "tags": []
},{
    "id": "24fc587b4f2334c3ee923e39f9141d6f",
    "title": "Gpu",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/gpu/",
    "relURI": "/gpu/",
    "body": " GPU # Nvidia # Ollama supports Nvidia GPUs with compute capability 5.0+.\nCheck your compute compatibility to see if your card is supported: https://developer.nvidia.com/cuda-gpus\nCompute Capability Family Cards 9.0 NVIDIA H100 8.9 GeForce RTX 40xx RTX 4090 RTX 4080 RTX 4070 Ti RTX 4060 Ti NVIDIA Professional L4 L40 RTX 6000 8.6 GeForce RTX 30xx RTX 3090 Ti RTX 3090 RTX 3080 Ti RTX 3080 RTX 3070 Ti RTX 3070 RTX 3060 Ti RTX 3060 NVIDIA Professional A40 RTX A6000 RTX A5000 RTX A4000 RTX A3000 RTX A2000 A10 A16 A2 8.0 NVIDIA A100 A30 7.5 GeForce GTX/RTX GTX 1650 Ti TITAN RTX RTX 2080 Ti RTX 2080 RTX 2070 RTX 2060 NVIDIA Professional T4 RTX 5000 RTX 4000 RTX 3000 T2000 T1200 T1000 T600 T500 Quadro RTX 8000 RTX 6000 RTX 5000 RTX 4000 7.0 NVIDIA TITAN V V100 Quadro GV100 6.1 NVIDIA TITAN TITAN Xp TITAN X GeForce GTX GTX 1080 Ti GTX 1080 GTX 1070 Ti GTX 1070 GTX 1060 GTX 1050 Quadro P6000 P5200 P4200 P3200 P5000 P4000 P3000 P2200 P2000 P1000 P620 P600 P500 P520 Tesla P40 P4 6.0 NVIDIA Tesla P100 Quadro GP100 5.2 GeForce GTX GTX TITAN X GTX 980 Ti GTX 980 GTX 970 GTX 960 GTX 950 Quadro M6000 24GB M6000 M5000 M5500M M4000 M2200 M2000 M620 Tesla M60 M40 5.0 GeForce GTX GTX 750 Ti GTX 750 NVS 810 Quadro K2200 K1200 K620 M1200 M520 M5000M M4000M M3000M M2000M M1000M K620M M600M M500M GPU Selection # If you have multiple NVIDIA GPUs in your system and want to limit Ollama to use a subset, you can set CUDA_VISIBLE_DEVICES to a comma separated list of GPUs. Numeric IDs may be used, however ordering may vary, so UUIDs are more reliable. You can discover the UUID of your GPUs by running nvidia-smi -L If you want to ignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \u0026ldquo;-1\u0026rdquo;)\nLaptop Suspend Resume # On linux, after a suspend/resume cycle, sometimes Ollama will fail to discover your NVIDIA GPU, and fallback to running on the CPU. You can workaround this driver bug by reloading the NVIDIA UVM driver with sudo rmmod nvidia_uvm \u0026amp;\u0026amp; sudo modprobe nvidia_uvm\nAMD Radeon # Ollama supports the following AMD GPUs:\nFamily Cards and accelerators AMD Radeon RX 7900 XTX 7900 XT 7900 GRE 7800 XT 7700 XT 7600 XT 7600 6950 XT 6900 XTX 6900XT 6800 XT 6800 Vega 64 Vega 56 AMD Radeon PRO W7900 W7800 W7700 W7600 W7500 W6900X W6800X Duo W6800X W6800 V620 V420 V340 V320 Vega II Duo Vega II VII SSG AMD Instinct MI300X MI300A MI300 MI250X MI250 MI210 MI200 MI100 MI60 MI50 Overrides # Ollama leverages the AMD ROCm library, which does not support all AMD GPUs. In some cases you can force the system to try to use a similar LLVM target that is close. For example The Radeon RX 5400 is gfx1034 (also known as 10.3.4) however, ROCm does not currently support this target. The closest support is gfx1030. You can use the environment variable HSA_OVERRIDE_GFX_VERSION with x.y.z syntax. So for example, to force the system to run on the RX 5400, you would set HSA_OVERRIDE_GFX_VERSION=\u0026quot;10.3.0\u0026quot; as an environment variable for the server. If you have an unsupported AMD GPU you can experiment using the list of supported types below.\nAt this time, the known supported GPU types are the following LLVM Targets. This table shows some example GPUs that map to these LLVM targets:\nLLVM Target An Example GPU gfx900 Radeon RX Vega 56 gfx906 Radeon Instinct MI50 gfx908 Radeon Instinct MI100 gfx90a Radeon Instinct MI210 gfx940 Radeon Instinct MI300 gfx941 gfx942 gfx1030 Radeon PRO V620 gfx1100 Radeon PRO W7900 gfx1101 Radeon PRO W7700 gfx1102 Radeon RX 7600 AMD is working on enhancing ROCm v6 to broaden support for families of GPUs in a future release which should increase support for more GPUs.\nReach out on Discord or file an issue for additional help.\nGPU Selection # If you have multiple AMD GPUs in your system and want to limit Ollama to use a subset, you can set HIP_VISIBLE_DEVICES to a comma separated list of GPUs. You can see the list of devices with rocminfo. If you want to ignore the GPUs and force CPU usage, use an invalid GPU ID (e.g., \u0026ldquo;-1\u0026rdquo;)\nContainer Permission # In some Linux distributions, SELinux can prevent containers from accessing the AMD GPU devices. On the host system you can run sudo setsebool container_use_devices=1 to allow containers to use devices.\nMetal (Apple GPUs) # Ollama supports GPU acceleration on Apple devices via the Metal API.\n",
    "tags": []
},{
    "id": "0e1a79a7a5b946c20b7cbae5d0a5c544",
    "title": "Import",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/import/",
    "relURI": "/import/",
    "body": " Import a model # This guide walks through importing a GGUF, PyTorch or Safetensors model.\nImporting (GGUF) # Step 1: Write a Modelfile # Start by creating a Modelfile. This file is the blueprint for your model, specifying weights, parameters, prompt templates and more.\nFROM ./mistral-7b-v0.1.Q4_0.gguf Copy (Optional) many chat models require a prompt template in order to answer correctly. A default prompt template can be specified with the TEMPLATE instruction in the Modelfile:\nFROM ./mistral-7b-v0.1.Q4_0.gguf TEMPLATE \u0026#34;[INST] {{ .Prompt }} [/INST]\u0026#34; Copy Step 2: Create the Ollama model # Finally, create a model from your Modelfile:\nollama create example -f Modelfile Copy Step 3: Run your model # Next, test the model with ollama run:\nollama run example \u0026#34;What is your favourite condiment?\u0026#34; Copy Importing (PyTorch \u0026amp; Safetensors) # Importing from PyTorch and Safetensors is a longer process than importing from GGUF. Improvements that make it easier are a work in progress.\nSetup # First, clone the ollama/ollama repo:\ngit clone git@github.com:ollama/ollama.git ollama cd ollama Copy and then fetch its llama.cpp submodule:\ngit submodule init git submodule update llm/llama.cpp Copy Next, install the Python dependencies:\npython3 -m venv llm/llama.cpp/.venv source llm/llama.cpp/.venv/bin/activate pip install -r llm/llama.cpp/requirements.txt Copy Then build the quantize tool:\nmake -C llm/llama.cpp quantize Copy Clone the HuggingFace repository (optional) # If the model is currently hosted in a HuggingFace repository, first clone that repository to download the raw model.\nInstall Git LFS, verify it\u0026rsquo;s installed, and then clone the model\u0026rsquo;s repository:\ngit lfs install git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 model Copy Convert the model # Note: some model architectures require using specific convert scripts. For example, Qwen models require running convert-hf-to-gguf.py instead of convert.py\npython llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin Copy Quantize the model # llm/llama.cpp/quantize converted.bin quantized.bin q4_0 Copy Step 3: Write a Modelfile # Next, create a Modelfile for your model:\nFROM quantized.bin TEMPLATE \u0026#34;[INST] {{ .Prompt }} [/INST]\u0026#34; Copy Step 4: Create the Ollama model # Finally, create a model from your Modelfile:\nollama create example -f Modelfile Copy Step 5: Run your model # Next, test the model with ollama run:\nollama run example \u0026#34;What is your favourite condiment?\u0026#34; Copy Publishing your model (optional  early alpha) # Publishing models is in early alpha. If you\u0026rsquo;d like to publish your model to share with others, follow these steps:\nCreate an account Copy your Ollama public key: macOS: cat ~/.ollama/id_ed25519.pub Windows: type %USERPROFILE%\\.ollama\\id_ed25519.pub Linux: cat /usr/share/ollama/.ollama/id_ed25519.pub Add your public key to your Ollama account Next, copy your model to your username\u0026rsquo;s namespace:\nollama cp example \u0026lt;your username\u0026gt;/example Copy Then push the model:\nollama push \u0026lt;your username\u0026gt;/example Copy After publishing, your model will be available at https://ollama.com/\u0026lt;your username\u0026gt;/example.\nQuantization reference # The quantization options are as follow (from highest highest to lowest levels of quantization). Note: some architectures such as Falcon do not support K quants.\nq2_K q3_K q3_K_S q3_K_M q3_K_L q4_0 (recommended) q4_1 q4_K q4_K_S q4_K_M q5_0 q5_1 q5_K q5_K_S q5_K_M q6_K q8_0 f16 ",
    "tags": []
},{
    "id": "34fe9f2d9cc3cd7e319ca75e7da08332",
    "title": "Linux",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/linux/",
    "relURI": "/linux/",
    "body": " Ollama on Linux # Install # Install Ollama running this one-liner:\ncurl -fsSL https://ollama.com/install.sh | sh Copy AMD Radeon GPU support # While AMD has contributed the amdgpu driver upstream to the official linux kernel source, the version is older and may not support all ROCm features. We recommend you install the latest driver from https://www.amd.com/en/support/linux-drivers for best support of your Radeon GPU.\nManual install # Download the ollama binary # Ollama is distributed as a self-contained binary. Download it to a directory in your PATH:\nsudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama sudo chmod +x /usr/bin/ollama Copy Adding Ollama as a startup service (recommended) # Create a user for Ollama:\nsudo useradd -r -s /bin/false -m -d /usr/share/ollama ollama Copy Create a service file in /etc/systemd/system/ollama.service:\n[Unit] Description=Ollama Service After=network-online.target [Service] ExecStart=/usr/bin/ollama serve User=ollama Group=ollama Restart=always RestartSec=3 [Install] WantedBy=default.target Copy Then start the service:\nsudo systemctl daemon-reload sudo systemctl enable ollama Copy Install CUDA drivers (optional  for Nvidia GPUs) # Download and install CUDA.\nVerify that the drivers are installed by running the following command, which should print details about your GPU:\nnvidia-smi Copy Install ROCm (optional - for Radeon GPUs) # Download and Install\nMake sure to install ROCm v6\nStart Ollama # Start Ollama using systemd:\nsudo systemctl start ollama Copy Update # Update ollama by running the install script again:\ncurl -fsSL https://ollama.com/install.sh | sh Copy Or by downloading the ollama binary:\nsudo curl -L https://ollama.com/download/ollama-linux-amd64 -o /usr/bin/ollama sudo chmod +x /usr/bin/ollama Copy Viewing logs # To view logs of Ollama running as a startup service, run:\njournalctl -u ollama Copy Uninstall # Remove the ollama service:\nsudo systemctl stop ollama sudo systemctl disable ollama sudo rm /etc/systemd/system/ollama.service Copy Remove the ollama binary from your bin directory (either /usr/local/bin, /usr/bin, or /bin):\nsudo rm $(which ollama) Copy Remove the downloaded models and Ollama service user and group:\nsudo rm -r /usr/share/ollama sudo userdel ollama sudo groupdel ollama Copy ",
    "tags": []
},{
    "id": "6c2093b5bf59582e4bd6dfe31e781290",
    "title": "Modelfile",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/modelfile/",
    "relURI": "/modelfile/",
    "body": " Ollama Model File # Note: Modelfile syntax is in development\nA model file is the blueprint to create and share models with Ollama.\nTable of Contents # Format Examples Instructions FROM (Required) Build from llama3 Build from a bin file PARAMETER Valid Parameters and Values TEMPLATE Template Variables SYSTEM ADAPTER LICENSE MESSAGE Notes Format # The format of the Modelfile:\n# comment INSTRUCTION arguments Copy Instruction Description FROM (required) Defines the base model to use. PARAMETER Sets the parameters for how Ollama will run the model. TEMPLATE The full prompt template to be sent to the model. SYSTEM Specifies the system message that will be set in the template. ADAPTER Defines the (Q)LoRA adapters to apply to the model. LICENSE Specifies the legal license. MESSAGE Specify message history. Examples # Basic Modelfile # An example of a Modelfile creating a mario blueprint:\nFROM llama3 # sets the temperature to 1 [higher is more creative, lower is more coherent] PARAMETER temperature 1 # sets the context window size to 4096, this controls how many tokens the LLM can use as context to generate the next token PARAMETER num_ctx 4096 # sets a custom system message to specify the behavior of the chat assistant SYSTEM You are Mario from super mario bros, acting as an assistant. Copy To use this:\nSave it as a file (e.g. Modelfile) ollama create choose-a-model-name -f \u0026lt;location of the file e.g. ./Modelfile\u0026gt;' ollama run choose-a-model-name Start using the model! More examples are available in the examples directory.\nTo view the Modelfile of a given model, use the ollama show --modelfile command.\n\u0026gt; ollama show --modelfile llama3 # Modelfile generated by \u0026#34;ollama show\u0026#34; # To build a new Modelfile based on this one, replace the FROM line with: # FROM llama3:latest FROM /Users/pdevine/.ollama/models/blobs/sha256-00e1317cbf74d901080d7100f57580ba8dd8de57203072dc6f668324ba545f29 TEMPLATE \u0026#34;\u0026#34;\u0026#34;{{ if .System }}\u0026lt;|start_header_id|\u0026gt;system\u0026lt;|end_header_id|\u0026gt; {{ .System }}\u0026lt;|eot_id|\u0026gt;{{ end }}{{ if .Prompt }}\u0026lt;|start_header_id|\u0026gt;user\u0026lt;|end_header_id|\u0026gt; {{ .Prompt }}\u0026lt;|eot_id|\u0026gt;{{ end }}\u0026lt;|start_header_id|\u0026gt;assistant\u0026lt;|end_header_id|\u0026gt; {{ .Response }}\u0026lt;|eot_id|\u0026gt;\u0026#34;\u0026#34;\u0026#34; PARAMETER stop \u0026#34;\u0026lt;|start_header_id|\u0026gt;\u0026#34; PARAMETER stop \u0026#34;\u0026lt;|end_header_id|\u0026gt;\u0026#34; PARAMETER stop \u0026#34;\u0026lt;|eot_id|\u0026gt;\u0026#34; PARAMETER stop \u0026#34;\u0026lt;|reserved_special_token\u0026#34; Copy Instructions # FROM (Required) # The FROM instruction defines the base model to use when creating a model.\nFROM \u0026lt;model name\u0026gt;:\u0026lt;tag\u0026gt; Copy Build from llama3 # FROM llama3 Copy A list of available base models: https://github.com/ollama/ollama#model-library\nBuild from a bin file # FROM ./ollama-model.bin Copy This bin file location should be specified as an absolute path or relative to the Modelfile location.\nPARAMETER # The PARAMETER instruction defines a parameter that can be set when the model is run.\nPARAMETER \u0026lt;parameter\u0026gt; \u0026lt;parametervalue\u0026gt; Copy Valid Parameters and Values # Parameter Description Value Type Example Usage mirostat Enable Mirostat sampling for controlling perplexity. (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) int mirostat 0 mirostat_eta Influences how quickly the algorithm responds to feedback from the generated text. A lower learning rate will result in slower adjustments, while a higher learning rate will make the algorithm more responsive. (Default: 0.1) float mirostat_eta 0.1 mirostat_tau Controls the balance between coherence and diversity of the output. A lower value will result in more focused and coherent text. (Default: 5.0) float mirostat_tau 5.0 num_ctx Sets the size of the context window used to generate the next token. (Default: 2048) int num_ctx 4096 repeat_last_n Sets how far back for the model to look back to prevent repetition. (Default: 64, 0 = disabled, -1 = num_ctx) int repeat_last_n 64 repeat_penalty Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g., 0.9) will be more lenient. (Default: 1.1) float repeat_penalty 1.1 temperature The temperature of the model. Increasing the temperature will make the model answer more creatively. (Default: 0.8) float temperature 0.7 seed Sets the random number seed to use for generation. Setting this to a specific number will make the model generate the same text for the same prompt. (Default: 0) int seed 42 stop Sets the stop sequences to use. When this pattern is encountered the LLM will stop generating text and return. Multiple stop patterns may be set by specifying multiple separate stop parameters in a modelfile. string stop \u0026ldquo;AI assistant:\u0026rdquo; tfs_z Tail free sampling is used to reduce the impact of less probable tokens from the output. A higher value (e.g., 2.0) will reduce the impact more, while a value of 1.0 disables this setting. (default: 1) float tfs_z 1 num_predict Maximum number of tokens to predict when generating text. (Default: 128, -1 = infinite generation, -2 = fill context) int num_predict 42 top_k Reduces the probability of generating nonsense. A higher value (e.g. 100) will give more diverse answers, while a lower value (e.g. 10) will be more conservative. (Default: 40) int top_k 40 top_p Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9) float top_p 0.9 TEMPLATE # TEMPLATE of the full prompt template to be passed into the model. It may include (optionally) a system message, a user\u0026rsquo;s message and the response from the model. Note: syntax may be model specific. Templates use Go template syntax.\nTemplate Variables # Variable Description {{ .System }} The system message used to specify custom behavior. {{ .Prompt }} The user prompt message. {{ .Response }} The response from the model. When generating a response, text after this variable is omitted. TEMPLATE \u0026#34;\u0026#34;\u0026#34;{{ if .System }}\u0026lt;|im_start|\u0026gt;system {{ .System }}\u0026lt;|im_end|\u0026gt; {{ end }}{{ if .Prompt }}\u0026lt;|im_start|\u0026gt;user {{ .Prompt }}\u0026lt;|im_end|\u0026gt; {{ end }}\u0026lt;|im_start|\u0026gt;assistant \u0026#34;\u0026#34;\u0026#34; Copy SYSTEM # The SYSTEM instruction specifies the system message to be used in the template, if applicable.\nSYSTEM \u0026#34;\u0026#34;\u0026#34;\u0026lt;system message\u0026gt;\u0026#34;\u0026#34;\u0026#34; Copy ADAPTER # The ADAPTER instruction is an optional instruction that specifies any LoRA adapter that should apply to the base model. The value of this instruction should be an absolute path or a path relative to the Modelfile and the file must be in a GGML file format. The adapter should be tuned from the base model otherwise the behaviour is undefined.\nADAPTER ./ollama-lora.bin Copy LICENSE # The LICENSE instruction allows you to specify the legal license under which the model used with this Modelfile is shared or distributed.\nLICENSE \u0026#34;\u0026#34;\u0026#34; \u0026lt;license text\u0026gt; \u0026#34;\u0026#34;\u0026#34; Copy MESSAGE # The MESSAGE instruction allows you to specify a message history for the model to use when responding. Use multiple iterations of the MESSAGE command to build up a conversation which will guide the model to answer in a similar way.\nMESSAGE \u0026lt;role\u0026gt; \u0026lt;message\u0026gt; Copy Valid roles # Role Description system Alternate way of providing the SYSTEM message for the model. user An example message of what the user could have asked. assistant An example message of how the model should respond. Example conversation # MESSAGE user Is Toronto in Canada? MESSAGE assistant yes MESSAGE user Is Sacramento in Canada? MESSAGE assistant no MESSAGE user Is Ontario in Canada? MESSAGE assistant yes Copy Notes # the Modelfile is not case sensitive. In the examples, uppercase instructions are used to make it easier to distinguish it from arguments. Instructions can be in any order. In the examples, the FROM instruction is first to keep it easily readable. ",
    "tags": []
},{
    "id": "3c125f0f2de25aea767048f0c27405fc",
    "title": "Openai",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/openai/",
    "relURI": "/openai/",
    "body": " OpenAI compatibility # Note: OpenAI compatibility is experimental and is subject to major adjustments including breaking changes. For fully-featured access to the Ollama API, see the Ollama Python library, JavaScript library and REST API.\nOllama provides experimental compatibility with parts of the OpenAI API to help connect existing applications to Ollama.\nUsage # OpenAI Python library # from openai import OpenAI client = OpenAI( base_url=\u0026#39;http://localhost:11434/v1/\u0026#39;, # required but ignored api_key=\u0026#39;ollama\u0026#39;, ) chat_completion = client.chat.completions.create( messages=[ { \u0026#39;role\u0026#39;: \u0026#39;user\u0026#39;, \u0026#39;content\u0026#39;: \u0026#39;Say this is a test\u0026#39;, } ], model=\u0026#39;llama3\u0026#39;, ) Copy OpenAI JavaScript library # import OpenAI from \u0026#39;openai\u0026#39; const openai = new OpenAI({ baseURL: \u0026#39;http://localhost:11434/v1/\u0026#39;, // required but ignored apiKey: \u0026#39;ollama\u0026#39;, }) const chatCompletion = await openai.chat.completions.create({ messages: [{ role: \u0026#39;user\u0026#39;, content: \u0026#39;Say this is a test\u0026#39; }], model: \u0026#39;llama3\u0026#39;, }) Copy curl # curl http://localhost:11434/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;system\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;You are a helpful assistant.\u0026#34; }, { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34; } ] }\u0026#39; Copy Endpoints # /v1/chat/completions # Supported features # Chat completions Streaming JSON mode Reproducible outputs Vision Function calling Logprobs Supported request fields # model messages Text content Array of content parts frequency_penalty presence_penalty response_format seed stop stream temperature top_p max_tokens logit_bias tools tool_choice user n Notes # Setting seed will always set temperature to 0 finish_reason will always be stop usage.prompt_tokens will be 0 for completions where prompt evaluation is cached Models # Before using a model, pull it locally ollama pull:\nollama pull llama3 Copy Default model names # For tooling that relies on default OpenAI model names such as gpt-3.5-turbo, use ollama cp to copy an existing model name to a temporary name:\nollama cp llama3 gpt-3.5-turbo Copy Afterwards, this new model name can be specified the model field:\ncurl http://localhost:11434/v1/chat/completions \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;gpt-3.5-turbo\u0026#34;, \u0026#34;messages\u0026#34;: [ { \u0026#34;role\u0026#34;: \u0026#34;user\u0026#34;, \u0026#34;content\u0026#34;: \u0026#34;Hello!\u0026#34; } ] }\u0026#39; Copy ",
    "tags": []
},{
    "id": "04c6e90faac2675aa89e2176d2eec7d8",
    "title": "Readme",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/readme/",
    "relURI": "/readme/",
    "body": " Documentation # Getting Started # Quickstart Examples Importing models Linux Documentation Windows Documentation Docker Documentation Reference # API Reference Modelfile Reference OpenAI Compatibility Resources # Troubleshooting Guide FAQ Development guide ",
    "tags": []
},{
    "id": "73654fb858684160f86a6b76afb2a6fd",
    "title": "Troubleshooting",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/troubleshooting/",
    "relURI": "/troubleshooting/",
    "body": " How to troubleshoot issues # Sometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on Mac by running the command:\ncat ~/.ollama/logs/server.log Copy On Linux systems with systemd, the logs can be found with this command:\njournalctl -u ollama Copy When you run Ollama in a container, the logs go to stdout/stderr in the container:\ndocker logs \u0026lt;container-name\u0026gt; Copy (Use docker ps to find the container name)\nIf manually running ollama serve in a terminal, the logs will be on that terminal.\nWhen you run Ollama on Windows, there are a few different locations. You can view them in the explorer window by hitting \u0026lt;cmd\u0026gt;+R and type in:\nexplorer %LOCALAPPDATA%\\Ollama to view logs explorer %LOCALAPPDATA%\\Programs\\Ollama to browse the binaries (The installer adds this to your user PATH) explorer %HOMEPATH%\\.ollama to browse where models and configuration is stored explorer %TEMP% where temporary executable files are stored in one or more ollama* directories To enable additional debug logging to help troubleshoot problems, first Quit the running app from the tray menu then in a powershell terminal\n$env:OLLAMA_DEBUG=\u0026#34;1\u0026#34; \u0026amp; \u0026#34;ollama app.exe\u0026#34; Copy Join the Discord for help interpreting the logs.\nLLM libraries # Ollama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. cpu_avx2 will perform the best, followed by cpu_avx an the slowest but most compatible is cpu. Rosetta emulation under MacOS will work with the cpu library.\nIn the server log, you will see a message that looks something like this (varies from release to release):\nDynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5] Copy Experimental LLM Library Override\nYou can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:\nOLLAMA_LLM_LIBRARY=\u0026#34;cpu_avx2\u0026#34; ollama serve Copy You can see what features your CPU has with the following.\ncat /proc/cpuinfo| grep flags | head -1 Copy Installing older or pre-release versions on Linux # If you run into problems on Linux and want to install an older version, or you\u0026rsquo;d like to try out a pre-release before it\u0026rsquo;s officially released, you can tell the install script which version to install.\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=\u0026#34;0.1.29\u0026#34; sh Copy Linux tmp noexec # If your system is configured with the \u0026ldquo;noexec\u0026rdquo; flag where Ollama stores its temporary executable files, you can specify an alternate location by setting OLLAMA_TMPDIR to a location writable by the user ollama runs as. For example OLLAMA_TMPDIR=/usr/share/ollama/\n",
    "tags": []
},{
    "id": "2f9666f86bd8527efa1103fc382863ca",
    "title": "Tutorials",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/",
    "relURI": "/tutorials/",
    "body": " Tutorials # Here is a list of ways you can use Ollama with other tools to build interesting applications.\nUsing LangChain with Ollama in JavaScript Using LangChain with Ollama in Python Running Ollama on NVIDIA Jetson Devices Also be sure to check out the examples directory for more ways to use Ollama.\n",
    "tags": []
},{
    "id": "416ae54ea39c6f0d356a0de228eae5a6",
    "title": "Fly gpu",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/fly-gpu/",
    "relURI": "/tutorials/fly-gpu/",
    "body": " Running Ollama on Fly.io GPU Instances # Ollama runs with little to no configuration on Fly.io GPU instances. If you don\u0026rsquo;t have access to GPUs yet, you\u0026rsquo;ll need to apply for access on the waitlist. Once you\u0026rsquo;re accepted, you\u0026rsquo;ll get an email with instructions on how to get started.\nCreate a new app with fly apps create:\nfly apps create Copy Then create a fly.toml file in a new folder that looks like this:\napp = \u0026#34;sparkling-violet-709\u0026#34; primary_region = \u0026#34;ord\u0026#34; vm.size = \u0026#34;a100-40gb\u0026#34; # see https://fly.io/docs/gpus/gpu-quickstart/ for more info [build] image = \u0026#34;ollama/ollama\u0026#34; [http_service] internal_port = 11434 force_https = false auto_stop_machines = true auto_start_machines = true min_machines_running = 0 processes = [\u0026#34;app\u0026#34;] [mounts] source = \u0026#34;models\u0026#34; destination = \u0026#34;/root/.ollama\u0026#34; initial_size = \u0026#34;100gb\u0026#34; Copy Then create a new private IPv6 address for your app:\nfly ips allocate-v6 --private Copy Then deploy your app:\nfly deploy Copy And finally you can access it interactively with a new Fly.io Machine:\nfly machine run -e OLLAMA_HOST=http://your-app-name.flycast --shell ollama/ollama Copy $ ollama run openchat:7b-v3.5-fp16 \u0026gt;\u0026gt;\u0026gt; How do I bake chocolate chip cookies? To bake chocolate chip cookies, follow these steps: 1. Preheat the oven to 375F (190C) and line a baking sheet with parchment paper or silicone baking mat. 2. In a large bowl, mix together 1 cup of unsalted butter (softened), 3/4 cup granulated sugar, and 3/4 cup packed brown sugar until light and fluffy. 3. Add 2 large eggs, one at a time, to the butter mixture, beating well after each addition. Stir in 1 teaspoon of pure vanilla extract. 4. In a separate bowl, whisk together 2 cups all-purpose flour, 1/2 teaspoon baking soda, and 1/2 teaspoon salt. Gradually add the dry ingredients to the wet ingredients, stirring until just combined. 5. Fold in 2 cups of chocolate chips (or chunks) into the dough. 6. Drop rounded tablespoons of dough onto the prepared baking sheet, spacing them about 2 inches apart. 7. Bake for 10-12 minutes, or until the edges are golden brown. The centers should still be slightly soft. 8. Allow the cookies to cool on the baking sheet for a few minutes before transferring them to a wire rack to cool completely. Enjoy your homemade chocolate chip cookies! Copy When you set it up like this, it will automatically turn off when you\u0026rsquo;re done using it. Then when you access it again, it will automatically turn back on. This is a great way to save money on GPU instances when you\u0026rsquo;re not using them. If you want a persistent wake-on-use connection to your Ollama instance, you can set up a connection to your Fly network using WireGuard. Then you can access your Ollama instance at http://your-app-name.flycast.\nAnd that\u0026rsquo;s it!\n",
    "tags": []
},{
    "id": "038459ec4912cf57382c9cc466df6e99",
    "title": "Langchainjs",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/langchainjs/",
    "relURI": "/tutorials/langchainjs/",
    "body": " Using LangChain with Ollama using JavaScript # In this tutorial, we are going to use JavaScript with LangChain and Ollama to learn about something just a touch more recent. In August 2023, there was a series of wildfires on Maui. There is no way an LLM trained before that time can know about this, since their training data would not include anything as recent as that. So we can find the Wikipedia article about the fires and ask questions about the contents.\nTo get started, let\u0026rsquo;s just use LangChain to ask a simple question to a model. To do this with JavaScript, we need to install LangChain:\nnpm install langchain Copy Now we can start building out our JavaScript:\nimport { Ollama } from \u0026#34;langchain/llms/ollama\u0026#34;; const ollama = new Ollama({ baseUrl: \u0026#34;http://localhost:11434\u0026#34;, model: \u0026#34;llama3\u0026#34;, }); const answer = await ollama.invoke(`why is the sky blue?`); console.log(answer); Copy That will get us the same thing as if we ran ollama run llama3 \u0026quot;why is the sky blue\u0026quot; in the terminal. But we want to load a document from the web to ask a question against. Cheerio is a great library for ingesting a webpage, and LangChain uses it in their CheerioWebBaseLoader. So let\u0026rsquo;s install Cheerio and build that part of the app.\nnpm install cheerio Copy import { CheerioWebBaseLoader } from \u0026#34;langchain/document_loaders/web/cheerio\u0026#34;; const loader = new CheerioWebBaseLoader(\u0026#34;https://en.wikipedia.org/wiki/2023_Hawaii_wildfires\u0026#34;); const data = await loader.load(); Copy That will load the document. Although this page is smaller than the Odyssey, it is certainly bigger than the context size for most LLMs. So we are going to need to split into smaller pieces, and then select just the pieces relevant to our question. This is a great use for a vector datastore. In this example, we will use the MemoryVectorStore that is part of LangChain. But there is one more thing we need to get the content into the datastore. We have to run an embeddings process that converts the tokens in the text into a series of vectors. And for that, we are going to use Tensorflow. There is a lot of stuff going on in this one. First, install the Tensorflow components that we need.\nnpm install @tensorflow/tfjs-core@3.6.0 @tensorflow/tfjs-converter@3.6.0 @tensorflow-models/universal-sentence-encoder@1.3.3 @tensorflow/tfjs-node@4.10.0 Copy If you just install those components without the version numbers, it will install the latest versions, but there are conflicts within Tensorflow, so you need to install the compatible versions.\nimport { RecursiveCharacterTextSplitter } from \u0026#34;langchain/text_splitter\u0026#34; import { MemoryVectorStore } from \u0026#34;langchain/vectorstores/memory\u0026#34;; import \u0026#34;@tensorflow/tfjs-node\u0026#34;; import { TensorFlowEmbeddings } from \u0026#34;langchain/embeddings/tensorflow\u0026#34;; // Split the text into 500 character chunks. And overlap each chunk by 20 characters const textSplitter = new RecursiveCharacterTextSplitter({ chunkSize: 500, chunkOverlap: 20 }); const splitDocs = await textSplitter.splitDocuments(data); // Then use the TensorFlow Embedding to store these chunks in the datastore const vectorStore = await MemoryVectorStore.fromDocuments(splitDocs, new TensorFlowEmbeddings()); Copy To connect the datastore to a question asked to a LLM, we need to use the concept at the heart of LangChain: the chain. Chains are a way to connect a number of activities together to accomplish a particular tasks. There are a number of chain types available, but for this tutorial we are using the RetrievalQAChain.\nimport { RetrievalQAChain } from \u0026#34;langchain/chains\u0026#34;; const retriever = vectorStore.asRetriever(); const chain = RetrievalQAChain.fromLLM(ollama, retriever); const result = await chain.call({query: \u0026#34;When was Hawaii\u0026#39;s request for a major disaster declaration approved?\u0026#34;}); console.log(result.text) Copy So we created a retriever, which is a way to return the chunks that match a query from a datastore. And then connect the retriever and the model via a chain. Finally, we send a query to the chain, which results in an answer using our document as a source. The answer it returned was correct, August 10, 2023.\nAnd that is a simple introduction to what you can do with LangChain and Ollama.\n",
    "tags": []
},{
    "id": "545bd5b72d44b172679bfcc7ceef04f3",
    "title": "Langchainpy",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/langchainpy/",
    "relURI": "/tutorials/langchainpy/",
    "body": " Using LangChain with Ollama in Python # Let\u0026rsquo;s imagine we are studying the classics, such as the Odyssey by Homer. We might have a question about Neleus and his family. If you ask llama2 for that info, you may get something like:\nI apologize, but I\u0026rsquo;m a large language model, I cannot provide information on individuals or families that do not exist in reality. Neleus is not a real person or character, and therefore does not have a family or any other personal details. My apologies for any confusion. Is there anything else I can help you with?\nThis sounds like a typical censored response, but even llama2-uncensored gives a mediocre answer:\nNeleus was a legendary king of Pylos and the father of Nestor, one of the Argonauts. His mother was Clymene, a sea nymph, while his father was Neptune, the god of the sea.\nSo let\u0026rsquo;s figure out how we can use LangChain with Ollama to ask our question to the actual document, the Odyssey by Homer, using Python.\nLet\u0026rsquo;s start by asking a simple question that we can get an answer to from the Llama2 model using Ollama. First, we need to install the LangChain package:\npip install langchain\nThen we can create a model and ask the question:\nfrom langchain_community.llms import Ollama ollama = Ollama( base_url=\u0026#39;http://localhost:11434\u0026#39;, model=\u0026#34;llama3\u0026#34; ) print(ollama.invoke(\u0026#34;why is the sky blue\u0026#34;)) Copy Notice that we are defining the model and the base URL for Ollama.\nNow let\u0026rsquo;s load a document to ask questions against. I\u0026rsquo;ll load up the Odyssey by Homer, which you can find at Project Gutenberg. We will need WebBaseLoader which is part of LangChain and loads text from any webpage. On my machine, I also needed to install bs4 to get that to work, so run pip install bs4.\nfrom langchain.document_loaders import WebBaseLoader loader = WebBaseLoader(\u0026#34;https://www.gutenberg.org/files/1727/1727-h/1727-h.htm\u0026#34;) data = loader.load() Copy This file is pretty big. Just the preface is 3000 tokens. Which means the full document won\u0026rsquo;t fit into the context for the model. So we need to split it up into smaller pieces.\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter text_splitter=RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=0) all_splits = text_splitter.split_documents(data) Copy It\u0026rsquo;s split up, but we have to find the relevant splits and then submit those to the model. We can do this by creating embeddings and storing them in a vector database. We can use Ollama directly to instantiate an embedding model. We will use ChromaDB in this example for a vector database. pip install chromadb\nfrom langchain.embeddings import OllamaEmbeddings from langchain.vectorstores import Chroma oembed = OllamaEmbeddings(base_url=\u0026#34;http://localhost:11434\u0026#34;, model=\u0026#34;nomic-embed-text\u0026#34;) vectorstore = Chroma.from_documents(documents=all_splits, embedding=oembed) Copy Now let\u0026rsquo;s ask a question from the document. Who was Neleus, and who is in his family? Neleus is a character in the Odyssey, and the answer can be found in our text.\nquestion=\u0026#34;Who is Neleus and who is in Neleus\u0026#39; family?\u0026#34; docs = vectorstore.similarity_search(question) len(docs) Copy This will output the number of matches for chunks of data similar to the search.\nThe next thing is to send the question and the relevant parts of the docs to the model to see if we can get a good answer. But we are stitching two parts of the process together, and that is called a chain. This means we need to define a chain:\nfrom langchain.chains import RetrievalQA qachain=RetrievalQA.from_chain_type(ollama, retriever=vectorstore.as_retriever()) qachain.invoke({\u0026#34;query\u0026#34;: question}) Copy The answer received from this chain was:\nNeleus is a character in Homer\u0026rsquo;s \u0026ldquo;Odyssey\u0026rdquo; and is mentioned in the context of Penelope\u0026rsquo;s suitors. Neleus is the father of Chloris, who is married to Neleus and bears him several children, including Nestor, Chromius, Periclymenus, and Pero. Amphinomus, the son of Nisus, is also mentioned as a suitor of Penelope and is known for his good natural disposition and agreeable conversation.\nIt\u0026rsquo;s not a perfect answer, as it implies Neleus married his daughter when actually Chloris \u0026ldquo;was the youngest daughter to Amphion son of Iasus and king of Minyan Orchomenus, and was Queen in Pylos\u0026rdquo;.\nI updated the chunk_overlap for the text splitter to 20 and tried again and got a much better answer:\nNeleus is a character in Homer\u0026rsquo;s epic poem \u0026ldquo;The Odyssey.\u0026rdquo; He is the husband of Chloris, who is the youngest daughter of Amphion son of Iasus and king of Minyan Orchomenus. Neleus has several children with Chloris, including Nestor, Chromius, Periclymenus, and Pero.\nAnd that is a much better answer.\n",
    "tags": []
},{
    "id": "a3edd3036b5d08d0a043d0f7659a0cdf",
    "title": "Nvidia jetson",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"tutorials",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/tutorials/nvidia-jetson/",
    "relURI": "/tutorials/nvidia-jetson/",
    "body": " Running Ollama on NVIDIA Jetson Devices # Ollama runs well on NVIDIA Jetson Devices and should run out of the box with the standard installation instructions.\nThe following has been tested on JetPack 5.1.2, but should also work on JetPack 6.0.\nInstall Ollama via standard Linux command (ignore the 404 error): curl https://ollama.com/install.sh | sh Pull the model you want to use (e.g. mistral): ollama pull mistral Start an interactive session: ollama run mistral And that\u0026rsquo;s it!\nRunning Ollama in Docker # When running GPU accelerated applications in Docker, it is highly recommended to use dusty-nv jetson-containers repo.\n",
    "tags": []
},{
    "id": "3e44722f300b9d926b8a94aa7aeb0fd0",
    "title": "Windows",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/windows/",
    "relURI": "/windows/",
    "body": "\u0026laquo;\u0026laquo;\u0026laquo;\u0026lt; HEAD\nOllama Windows Preview # Welcome to the Ollama Windows preview.\nNo more WSL required!\nOllama now runs as a native Windows application, including NVIDIA and AMD Radeon GPU support. After installing Ollama Windows Preview, Ollama will run in the background and the ollama command line is available in cmd, powershell or your favorite terminal application. As usual the Ollama api will be served on http://localhost:11434.\nAs this is a preview release, you should expect a few bugs here and there. If you run into a problem you can reach out on Discord, or file an issue. Logs will often be helpful in diagnosing the problem (see Troubleshooting below)\nSystem Requirements # Windows 10 or newer, Home or Pro NVIDIA 452.39 or newer Drivers if you have an NVIDIA card AMD Radeon Driver https://www.amd.com/en/support if you have a Radeon card API Access # Here\u0026rsquo;s a quick example showing API access from powershell\n(Invoke-WebRequest -method POST -Body \u0026#39;{\u0026#34;model\u0026#34;:\u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;:\u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;stream\u0026#34;: false}\u0026#39; -uri http://localhost:11434/api/generate ).Content | ConvertFrom-json Copy Troubleshooting # While we\u0026rsquo;re in preview, OLLAMA_DEBUG is always enabled, which adds a \u0026ldquo;view logs\u0026rdquo; menu item to the app, and increses logging for the GUI app and server.\nOllama on Windows stores files in a few different locations. You can view them in the explorer window by hitting \u0026lt;cmd\u0026gt;+R and type in:\nexplorer %LOCALAPPDATA%\\Ollama contains logs, and downloaded updates app.log contains logs from the GUI application server.log contains the server logs upgrade.log contains log output for upgrades explorer %LOCALAPPDATA%\\Programs\\Ollama contains the binaries (The installer adds this to your user PATH) explorer %HOMEPATH%\\.ollama contains models and configuration explorer %TEMP% contains temporary executable files in one or more ollama* directories ",
    "tags": []
}
]