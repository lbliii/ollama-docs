{
    "id": "73654fb858684160f86a6b76afb2a6fd",
    "title": "Troubleshooting",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/troubleshooting/",
    "relURI": "/troubleshooting/",
    "body": " How to troubleshoot issues # Sometimes Ollama may not perform as expected. One of the best ways to figure out what happened is to take a look at the logs. Find the logs on Mac by running the command:\ncat ~/.ollama/logs/server.log Copy On Linux systems with systemd, the logs can be found with this command:\njournalctl -u ollama Copy When you run Ollama in a container, the logs go to stdout/stderr in the container:\ndocker logs \u0026lt;container-name\u0026gt; Copy (Use docker ps to find the container name)\nIf manually running ollama serve in a terminal, the logs will be on that terminal.\nWhen you run Ollama on Windows, there are a few different locations. You can view them in the explorer window by hitting \u0026lt;cmd\u0026gt;+R and type in:\nexplorer %LOCALAPPDATA%\\Ollama to view logs explorer %LOCALAPPDATA%\\Programs\\Ollama to browse the binaries (The installer adds this to your user PATH) explorer %HOMEPATH%\\.ollama to browse where models and configuration is stored explorer %TEMP% where temporary executable files are stored in one or more ollama* directories To enable additional debug logging to help troubleshoot problems, first Quit the running app from the tray menu then in a powershell terminal\n$env:OLLAMA_DEBUG=\u0026#34;1\u0026#34; \u0026amp; \u0026#34;ollama app.exe\u0026#34; Copy Join the Discord for help interpreting the logs.\nLLM libraries # Ollama includes multiple LLM libraries compiled for different GPUs and CPU vector features. Ollama tries to pick the best one based on the capabilities of your system. If this autodetection has problems, or you run into other problems (e.g. crashes in your GPU) you can workaround this by forcing a specific LLM library. cpu_avx2 will perform the best, followed by cpu_avx an the slowest but most compatible is cpu. Rosetta emulation under MacOS will work with the cpu library.\nIn the server log, you will see a message that looks something like this (varies from release to release):\nDynamic LLM libraries [rocm_v6 cpu cpu_avx cpu_avx2 cuda_v11 rocm_v5] Copy Experimental LLM Library Override\nYou can set OLLAMA_LLM_LIBRARY to any of the available LLM libraries to bypass autodetection, so for example, if you have a CUDA card, but want to force the CPU LLM library with AVX2 vector support, use:\nOLLAMA_LLM_LIBRARY=\u0026#34;cpu_avx2\u0026#34; ollama serve Copy You can see what features your CPU has with the following.\ncat /proc/cpuinfo| grep flags | head -1 Copy Installing older or pre-release versions on Linux # If you run into problems on Linux and want to install an older version, or you\u0026rsquo;d like to try out a pre-release before it\u0026rsquo;s officially released, you can tell the install script which version to install.\ncurl -fsSL https://ollama.com/install.sh | OLLAMA_VERSION=\u0026#34;0.1.29\u0026#34; sh Copy Linux tmp noexec # If your system is configured with the \u0026ldquo;noexec\u0026rdquo; flag where Ollama stores its temporary executable files, you can specify an alternate location by setting OLLAMA_TMPDIR to a location writable by the user ollama runs as. For example OLLAMA_TMPDIR=/usr/share/ollama/\n",
    "tags": []
}