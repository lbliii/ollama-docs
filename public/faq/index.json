{
    "id": "4a1318da06786ac52626178ca960109e",
    "title": "Faq",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/faq/",
    "relURI": "/faq/",
    "body": " FAQ # How can I upgrade Ollama? # Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click \u0026ldquo;Restart to update\u0026rdquo; to apply the update. Updates can also be installed by downloading the latest version manually.\nOn Linux, re-run the install script:\ncurl -fsSL https://ollama.com/install.sh | sh Copy How can I view the logs? # Review the Troubleshooting docs for more about using logs.\nIs my GPU compatible with Ollama? # Please refer to the GPU docs.\nHow can I specify the context window size? # By default, Ollama uses a context window size of 2048 tokens.\nTo change this when using ollama run, use /set parameter:\n/set parameter num_ctx 4096 Copy When using the API, specify the num_ctx parameter:\ncurl http://localhost:11434/api/generate -d \u0026#39;{ \u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;prompt\u0026#34;: \u0026#34;Why is the sky blue?\u0026#34;, \u0026#34;options\u0026#34;: { \u0026#34;num_ctx\u0026#34;: 4096 } }\u0026#39; Copy How do I configure Ollama server? # Ollama server can be configured with environment variables.\nSetting environment variables on Mac # If Ollama is run as a macOS application, environment variables should be set using launchctl:\nFor each environment variable, call launchctl setenv.\nlaunchctl setenv OLLAMA_HOST \u0026#34;0.0.0.0\u0026#34; Copy Restart Ollama application.\nSetting environment variables on Linux # If Ollama is run as a systemd service, environment variables should be set using systemctl:\nEdit the systemd service by calling systemctl edit ollama.service. This will open an editor.\nFor each environment variable, add a line Environment under section [Service]:\n[Service] Environment=\u0026#34;OLLAMA_HOST=0.0.0.0\u0026#34; Copy Save and exit.\nReload systemd and restart Ollama:\nsystemctl daemon-reload systemctl restart ollama Copy Setting environment variables on Windows # On windows, Ollama inherits your user and system environment variables.\nFirst Quit Ollama by clicking on it in the task bar\nEdit system environment variables from the control panel\nEdit or create New variable(s) for your user account for OLLAMA_HOST, OLLAMA_MODELS, etc.\nClick OK/Apply to save\nRun ollama from a new terminal window\nHow can I expose Ollama on my network? # Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the OLLAMA_HOST environment variable.\nRefer to the section above for how to set environment variables on your platform.\nHow can I use Ollama with a proxy server? # Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:\nserver { listen 80; server_name example.com; # Replace with your domain or IP location / { proxy_pass http://localhost:11434; proxy_set_header Host localhost:11434; } } Copy How can I use Ollama with ngrok? # Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:\nngrok http 11434 --host-header=\u0026#34;localhost:11434\u0026#34; Copy How can I use Ollama with Cloudflare Tunnel? # To use Ollama with Cloudflare Tunnel, use the --url and --http-host-header flags:\ncloudflared tunnel --url http://localhost:11434 --http-host-header=\u0026#34;localhost:11434\u0026#34; Copy How can I allow additional web origins to access Ollama? # Ollama allows cross-origin requests from 127.0.0.1 and 0.0.0.0 by default. Additional origins can be configured with OLLAMA_ORIGINS.\nRefer to the section above for how to set environment variables on your platform.\nWhere are models stored? # macOS: ~/.ollama/models Linux: /usr/share/ollama/.ollama/models Windows: C:\\Users\\\u0026lt;username\u0026gt;\\.ollama\\models How do I set them to a different location? # If a different directory needs to be used, set the environment variable OLLAMA_MODELS to the chosen directory.\nRefer to the section above for how to set environment variables on your platform.\nDoes Ollama send my prompts and answers back to ollama.com? # No. Ollama runs locally, and conversation data does not leave your machine.\nHow can I use Ollama in Visual Studio Code? # There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of extensions \u0026amp; plugins at the bottom of the main repository readme.\nHow do I use Ollama behind a proxy? # Ollama is compatible with proxy servers if HTTP_PROXY or HTTPS_PROXY are configured. When using either variables, ensure it is set where ollama serve can access the values. When using HTTPS_PROXY, ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.\nHow do I use Ollama behind a proxy in Docker? # The Ollama Docker container image can be configured to use a proxy by passing -e HTTPS_PROXY=https://proxy.example.com when starting the container.\nAlternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on macOS, Windows, and Linux, and Docker daemon with systemd.\nEnsure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.\nFROM ollama/ollama COPY my-ca.pem /usr/local/share/ca-certificates/my-ca.crt RUN update-ca-certificates Copy Build and run this image:\ndocker build -t ollama-with-ca . docker run -d -e HTTPS_PROXY=https://my.proxy.example.com -p 11434:11434 ollama-with-ca Copy How do I use Ollama with GPU acceleration in Docker? # The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the nvidia-container-toolkit. See ollama/ollama for more details.\nGPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.\nWhy is networking slow in WSL2 on Windows 10? # This can impact both installing Ollama, as well as downloading models.\nOpen Control Panel \u0026gt; Networking and Internet \u0026gt; View network status and tasks and click on Change adapter settings on the left panel. Find the vEthernel (WSL) adapter, right click and select Properties. Click on Configure and open the Advanced tab. Search through each of the properties until you find Large Send Offload Version 2 (IPv4) and Large Send Offload Version 2 (IPv6). Disable both of these properties.\nHow can I pre-load a model to get faster response times? # If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the /api/generate and /api/chat API endpoints.\nTo preload the mistral model using the generate endpoint, use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;}\u0026#39; Copy To use the chat completions endpoint, use:\ncurl http://localhost:11434/api/chat -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;mistral\u0026#34;}\u0026#39; Copy How do I keep a model loaded in memory or make it unload immediately? # By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you are making numerous requests to the LLM. You may, however, want to free up the memory before the 5 minutes have elapsed or keep the model loaded indefinitely. Use the keep_alive parameter with either the /api/generate and /api/chat API endpoints to control how long the model is left in memory.\nThe keep_alive parameter can be set to:\na duration string (such as \u0026ldquo;10m\u0026rdquo; or \u0026ldquo;24h\u0026rdquo;) a number in seconds (such as 3600) any negative number which will keep the model loaded in memory (e.g. -1 or \u0026ldquo;-1m\u0026rdquo;) \u0026lsquo;0\u0026rsquo; which will unload the model immediately after generating a response For example, to preload a model and leave it in memory use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;keep_alive\u0026#34;: -1}\u0026#39; Copy To unload the model and free up memory use:\ncurl http://localhost:11434/api/generate -d \u0026#39;{\u0026#34;model\u0026#34;: \u0026#34;llama3\u0026#34;, \u0026#34;keep_alive\u0026#34;: 0}\u0026#39; Copy Alternatively, you can change the amount of time all models are loaded into memory by setting the OLLAMA_KEEP_ALIVE environment variable when starting the Ollama server. The OLLAMA_KEEP_ALIVE variable uses the same parameter types as the keep_alive parameter types mentioned above. Refer to section explaining how to configure the Ollama server to correctly set the environment variable.\nIf you wish to override the OLLAMA_KEEP_ALIVE setting, use the keep_alive API parameter with the /api/generate or /api/chat API endpoints.\n",
    "tags": []
}