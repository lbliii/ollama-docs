<!DOCTYPE html>
<html lang="en-us" dir="ltr">
<head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width">
<meta name="title" content="MiloDocs: " />
<meta name="description" content="FAQ # How can I upgrade Ollama? # Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click &ldquo;Restart to update&rdquo; to apply the update. Updates can also be installed by downloading the latest version manually.
On Linux, re-run the install script:
curl -fsSL https://ollama.com/install.sh | sh Copy How can I view the logs? # Review the Troubleshooting docs for more about using logs." />
<meta name="keywords" content="[]" />
<meta name="author" content="" />


<title> | MiloDocs</title>

  <link rel="stylesheet" href="/css/bundle.css">
<script src="/js/bundle.cca7746894b8b9b1c4052fd8b832a8e218fe49bcae077ac61493efc3f7ab4e71.js" integrity="sha256-zKd0aJS4ubHEBS/YuDKo4hj&#43;SbyuB3rGFJPvw/erTnE=" crossorigin="anonymous"></script>

</head>
<body class="bg-white">
  <header>
</header><nav id="topNav" class="sticky top-0 z-50 py-2 bg-white">
    <div class="max-w-screen-xl 2xl:max-w-screen-2xl mx-auto">
        <div id="topNavItemsContainer" class="flex space-x-2">
            <a href="/" class="flex items-center">
    <span class="w-5 h-5">
            <img src="/images/milo.png" alt="Logo Small" class="w-5"></span>
</a>

<div id="topNavLinks" class="flex items-center mx-2 space-x-4 text-black transition duration-300 justify-start w-1/3 font-brand font-light text-sm">
        <a href="/" aria-label="Home" class="hidden md:block">Home</a><a href="/get-started/" aria-label="Get Started" class="hidden md:block">Get Started</a>
    
    <div id="topNavPrimaryDropdown" class="hidden md:block relative group z-50">
        <a href="/" aria-label="Dropdown" class="">Dropdown</a>
        <div class="absolute left-0 mt-0 w-48 bg-white border border-gray-200 rounded-md overflow-hidden shadow-lg group-hover:block hidden p-1"><a href="/about" aria-label="About Us" class="flex px-4 py-1 text-sm text-black hover:bg-zinc-100 transition duration-100">About Us</a><a href="/services" aria-label="Services" class="flex px-4 py-1 text-sm text-black hover:bg-zinc-100 transition duration-100">Services</a><a href="/contact" aria-label="Contact" class="flex px-4 py-1 text-sm text-black hover:bg-zinc-100 transition duration-100">Contact</a></div>
    </div></div>
<div id="topNavSearch" class="flex items-center space-x-4 text-xs">
    <input type="search" id="searchInput" class="border rounded-lg p-2 focus:outline-none focus:ring-2 focus:ring-brand md:w-96 bg-zinc-100 text-black" placeholder="Search..." aria-label="Search" />
</div><div id="topNavEnd" class="flex w-1/3 justify-end">
                
                <div class="flex items-center space-x-4">
                    <button id="chatTocToggle" aria-label="Toggle Chat" class="hidden md:block"><span class="w-5 h-5"><img id="chatRob" src="/icons/robot.svg" alt="toggle" class="hidden" title="Chat Panel"><img id="chatToc" src="/icons/toc.svg" alt="toggle" class="hidden" title="TOC Panel"></span></button>
                    <button id="darkModeToggle" aria-label="Toggle Darkmode"><span class="w-5 h-5"><img id="moon" src="/icons/dark.svg" alt="moon" class="hidden" title="Dark Mode"><img id="sun" src="/icons/light.svg" alt="sun" class="hidden" title="Light Mode"></span></button>
                </div>
            </div>
        </div>
    </div>
</nav><main class="max-w-screen-xl 2xl:max-w-screen-2xl mx-auto flex"><aside id="sidebar-left" class="hidden md:flex md:translate-x-0 w-60 text-black">
    <div id="linkTree" class="sticky top-10 pt-4 h-[calc(100vh-4rem)] overflow-y-auto w-full">
        
        
        
    </div>
</aside>




<div id="pageContainer" class="w-full lg:w-3/5"><section id="articleSummarizationContainer" class="hidden border border-dotted m-4 p-4 rounded-md transition-opacity duration-500">
    <h2 class="text-3xl font-bold text-black mb-4">Article Summarization</h2>
    <div id="articleSummaryOutput" class="flex items-center justify-center p-4 my-4 bg-zinc-100 rounded-md text-black text-sm transition-all duration-300 ease-in-out">
        <div class="animate-spin h-5 w-5 mr-3 border-4 border-brand rounded-full border-t-transparent"></div>
        <p id="generationMessage">Generating summary...</p>
    </div>
</section>

  <div class="my-4">
    <nav aria-label="Breadcrumb Navigation" class="breadcrumb flex flex-wrap text-xs px-2 items-center md:px-4">
  <ol class="flex space-x-2 md:space-x-4 lg:space-x-6">
    
    <li>
      <a href="//localhost:1313/faq/" class="text-black px-2 py-1 md:px-4 md:py-2 lg:px-6 rounded shadow-md block tile transition duration-300" aria-label="Breadcrumb Item" aria-current="page">
          
      </a>
    </li>
  </ol>
</nav>

  </div>
  <h1 class="text-black p-4"></h1>
  <div id="articleContent" class="p-4">
    



<h1 id="faq" class="group block ">
    FAQ
    <a id="-faq" href="#-faq" class="hidden group-hover:inline text-current ml-2">#</a>
</h1>



<h2 id="how-can-i-upgrade-ollama" class="group block py-4">
    How can I upgrade Ollama?
    <a id="-how-can-i-upgrade-ollama" href="#-how-can-i-upgrade-ollama" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama on macOS and Windows will automatically download updates. Click on the taskbar or menubar item and then click &ldquo;Restart to update&rdquo; to apply the update. Updates can also be installed by downloading the latest version <a href="https://ollama.com/download/">manually</a>.</p>
<p>On Linux, re-run the install script:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>curl -fsSL https://ollama.com/install.sh | sh</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-can-i-view-the-logs" class="group block py-4">
    How can I view the logs?
    <a id="-how-can-i-view-the-logs" href="#-how-can-i-view-the-logs" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Review the <a href="./troubleshooting.md">Troubleshooting</a> docs for more about using logs.</p>




<h2 id="is-my-gpu-compatible-with-ollama" class="group block py-4">
    Is my GPU compatible with Ollama?
    <a id="-is-my-gpu-compatible-with-ollama" href="#-is-my-gpu-compatible-with-ollama" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Please refer to the <a href="./gpu.md">GPU docs</a>.</p>




<h2 id="how-can-i-specify-the-context-window-size" class="group block py-4">
    How can I specify the context window size?
    <a id="-how-can-i-specify-the-context-window-size" href="#-how-can-i-specify-the-context-window-size" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>By default, Ollama uses a context window size of 2048 tokens.</p>
<p>To change this when using <code>ollama run</code>, use <code>/set parameter</code>:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>/set parameter num_ctx 4096</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

<p>When using the API, specify the <code>num_ctx</code> parameter:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>curl http://localhost:11434/api/generate -d &#39;{
  &#34;model&#34;: &#34;llama3&#34;,
  &#34;prompt&#34;: &#34;Why is the sky blue?&#34;,
  &#34;options&#34;: {
    &#34;num_ctx&#34;: 4096
  }
}&#39;</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-do-i-configure-ollama-server" class="group block py-4">
    How do I configure Ollama server?
    <a id="-how-do-i-configure-ollama-server" href="#-how-do-i-configure-ollama-server" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama server can be configured with environment variables.</p>




<h3 id="setting-environment-variables-on-mac" class="group block py-4">
    Setting environment variables on Mac
    <a id="-setting-environment-variables-on-mac" href="#-setting-environment-variables-on-mac" class="hidden group-hover:inline text-current ml-2">#</a>
</h3><p>If Ollama is run as a macOS application, environment variables should be set using <code>launchctl</code>:</p>
<ol>
<li>
<p>For each environment variable, call <code>launchctl setenv</code>.</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>launchctl setenv OLLAMA_HOST <span style="color:#e6db74">&#34;0.0.0.0&#34;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

</li>
<li>
<p>Restart Ollama application.</p>
</li>
</ol>




<h3 id="setting-environment-variables-on-linux" class="group block py-4">
    Setting environment variables on Linux
    <a id="-setting-environment-variables-on-linux" href="#-setting-environment-variables-on-linux" class="hidden group-hover:inline text-current ml-2">#</a>
</h3><p>If Ollama is run as a systemd service, environment variables should be set using <code>systemctl</code>:</p>
<ol>
<li>
<p>Edit the systemd service by calling <code>systemctl edit ollama.service</code>. This will open an editor.</p>
</li>
<li>
<p>For each environment variable, add a line <code>Environment</code> under section <code>[Service]</code>:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-ini" data-lang="ini"><span style="display:flex;"><span><span style="color:#66d9ef">[Service]</span>
</span></span><span style="display:flex;"><span><span style="color:#a6e22e">Environment</span><span style="color:#f92672">=</span><span style="color:#e6db74">&#34;OLLAMA_HOST=0.0.0.0&#34;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

</li>
<li>
<p>Save and exit.</p>
</li>
<li>
<p>Reload <code>systemd</code> and restart Ollama:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-bash" data-lang="bash"><span style="display:flex;"><span>systemctl daemon-reload
</span></span><span style="display:flex;"><span>systemctl restart ollama</span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

</li>
</ol>




<h3 id="setting-environment-variables-on-windows" class="group block py-4">
    Setting environment variables on Windows
    <a id="-setting-environment-variables-on-windows" href="#-setting-environment-variables-on-windows" class="hidden group-hover:inline text-current ml-2">#</a>
</h3><p>On windows, Ollama inherits your user and system environment variables.</p>
<ol>
<li>
<p>First Quit Ollama by clicking on it in the task bar</p>
</li>
<li>
<p>Edit system environment variables from the control panel</p>
</li>
<li>
<p>Edit or create New variable(s) for your user account for <code>OLLAMA_HOST</code>, <code>OLLAMA_MODELS</code>, etc.</p>
</li>
<li>
<p>Click OK/Apply to save</p>
</li>
<li>
<p>Run <code>ollama</code> from a new terminal window</p>
</li>
</ol>




<h2 id="how-can-i-expose-ollama-on-my-network" class="group block py-4">
    How can I expose Ollama on my network?
    <a id="-how-can-i-expose-ollama-on-my-network" href="#-how-can-i-expose-ollama-on-my-network" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama binds 127.0.0.1 port 11434 by default. Change the bind address with the <code>OLLAMA_HOST</code> environment variable.</p>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>




<h2 id="how-can-i-use-ollama-with-a-proxy-server" class="group block py-4">
    How can I use Ollama with a proxy server?
    <a id="-how-can-i-use-ollama-with-a-proxy-server" href="#-how-can-i-use-ollama-with-a-proxy-server" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama runs an HTTP server and can be exposed using a proxy server such as Nginx. To do so, configure the proxy to forward requests and optionally set required headers (if not exposing Ollama on the network). For example, with Nginx:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>server {
    listen 80;
    server_name example.com;  # Replace with your domain or IP
    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host localhost:11434;
    }
}</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-can-i-use-ollama-with-ngrok" class="group block py-4">
    How can I use Ollama with ngrok?
    <a id="-how-can-i-use-ollama-with-ngrok" href="#-how-can-i-use-ollama-with-ngrok" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama can be accessed using a range of tools for tunneling tools. For example with Ngrok:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>ngrok http 11434 --host-header=&#34;localhost:11434&#34;</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-can-i-use-ollama-with-cloudflare-tunnel" class="group block py-4">
    How can I use Ollama with Cloudflare Tunnel?
    <a id="-how-can-i-use-ollama-with-cloudflare-tunnel" href="#-how-can-i-use-ollama-with-cloudflare-tunnel" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>To use Ollama with Cloudflare Tunnel, use the <code>--url</code> and <code>--http-host-header</code> flags:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <pre tabindex="0"><code>cloudflared tunnel --url http://localhost:11434 --http-host-header=&#34;localhost:11434&#34;</code></pre>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-can-i-allow-additional-web-origins-to-access-ollama" class="group block py-4">
    How can I allow additional web origins to access Ollama?
    <a id="-how-can-i-allow-additional-web-origins-to-access-ollama" href="#-how-can-i-allow-additional-web-origins-to-access-ollama" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama allows cross-origin requests from <code>127.0.0.1</code> and <code>0.0.0.0</code> by default. Additional origins can be configured with <code>OLLAMA_ORIGINS</code>.</p>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>




<h2 id="where-are-models-stored" class="group block py-4">
    Where are models stored?
    <a id="-where-are-models-stored" href="#-where-are-models-stored" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><ul>
<li>macOS: <code>~/.ollama/models</code></li>
<li>Linux: <code>/usr/share/ollama/.ollama/models</code></li>
<li>Windows: <code>C:\Users\&lt;username&gt;\.ollama\models</code></li>
</ul>




<h3 id="how-do-i-set-them-to-a-different-location" class="group block py-4">
    How do I set them to a different location?
    <a id="-how-do-i-set-them-to-a-different-location" href="#-how-do-i-set-them-to-a-different-location" class="hidden group-hover:inline text-current ml-2">#</a>
</h3><p>If a different directory needs to be used, set the environment variable <code>OLLAMA_MODELS</code> to the chosen directory.</p>
<p>Refer to the section <a href="#how-do-i-configure-ollama-server">above</a> for how to set environment variables on your platform.</p>




<h2 id="does-ollama-send-my-prompts-and-answers-back-to-ollamacom" class="group block py-4">
    Does Ollama send my prompts and answers back to ollama.com?
    <a id="-does-ollama-send-my-prompts-and-answers-back-to-ollamacom" href="#-does-ollama-send-my-prompts-and-answers-back-to-ollamacom" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>No. Ollama runs locally, and conversation data does not leave your machine.</p>




<h2 id="how-can-i-use-ollama-in-visual-studio-code" class="group block py-4">
    How can I use Ollama in Visual Studio Code?
    <a id="-how-can-i-use-ollama-in-visual-studio-code" href="#-how-can-i-use-ollama-in-visual-studio-code" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>There is already a large collection of plugins available for VSCode as well as other editors that leverage Ollama. See the list of <a href="https://github.com/ollama/ollama#extensions--plugins">extensions &amp; plugins</a> at the bottom of the main repository readme.</p>




<h2 id="how-do-i-use-ollama-behind-a-proxy" class="group block py-4">
    How do I use Ollama behind a proxy?
    <a id="-how-do-i-use-ollama-behind-a-proxy" href="#-how-do-i-use-ollama-behind-a-proxy" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>Ollama is compatible with proxy servers if <code>HTTP_PROXY</code> or <code>HTTPS_PROXY</code> are configured. When using either variables, ensure it is set where <code>ollama serve</code> can access the values. When using <code>HTTPS_PROXY</code>, ensure the proxy certificate is installed as a system certificate. Refer to the section above for how to use environment variables on your platform.</p>




<h3 id="how-do-i-use-ollama-behind-a-proxy-in-docker" class="group block py-4">
    How do I use Ollama behind a proxy in Docker?
    <a id="-how-do-i-use-ollama-behind-a-proxy-in-docker" href="#-how-do-i-use-ollama-behind-a-proxy-in-docker" class="hidden group-hover:inline text-current ml-2">#</a>
</h3><p>The Ollama Docker container image can be configured to use a proxy by passing <code>-e HTTPS_PROXY=https://proxy.example.com</code> when starting the container.</p>
<p>Alternatively, the Docker daemon can be configured to use a proxy. Instructions are available for Docker Desktop on <a href="https://docs.docker.com/desktop/settings/mac/#proxies">macOS</a>, <a href="https://docs.docker.com/desktop/settings/windows/#proxies">Windows</a>, and <a href="https://docs.docker.com/desktop/settings/linux/#proxies">Linux</a>, and Docker <a href="https://docs.docker.com/config/daemon/systemd/#httphttps-proxy">daemon with systemd</a>.</p>
<p>Ensure the certificate is installed as a system certificate when using HTTPS. This may require a new Docker image when using a self-signed certificate.</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-dockerfile" data-lang="dockerfile"><span style="display:flex;"><span><span style="color:#66d9ef">FROM</span><span style="color:#e6db74"> ollama/ollama</span><span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">COPY</span> my-ca.pem /usr/local/share/ca-certificates/my-ca.crt<span style="color:#960050;background-color:#1e0010">
</span></span></span><span style="display:flex;"><span><span style="color:#960050;background-color:#1e0010"></span><span style="color:#66d9ef">RUN</span> update-ca-certificates</span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

<p>Build and run this image:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>docker build -t ollama-with-ca .
</span></span><span style="display:flex;"><span>docker run -d -e HTTPS_PROXY<span style="color:#f92672">=</span>https://my.proxy.example.com -p 11434:11434 ollama-with-ca</span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-do-i-use-ollama-with-gpu-acceleration-in-docker" class="group block py-4">
    How do I use Ollama with GPU acceleration in Docker?
    <a id="-how-do-i-use-ollama-with-gpu-acceleration-in-docker" href="#-how-do-i-use-ollama-with-gpu-acceleration-in-docker" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>The Ollama Docker container can be configured with GPU acceleration in Linux or Windows (with WSL2). This requires the <a href="https://github.com/NVIDIA/nvidia-container-toolkit">nvidia-container-toolkit</a>. See <a href="https://hub.docker.com/r/ollama/ollama">ollama/ollama</a> for more details.</p>
<p>GPU acceleration is not available for Docker Desktop in macOS due to the lack of GPU passthrough and emulation.</p>




<h2 id="why-is-networking-slow-in-wsl2-on-windows-10" class="group block py-4">
    Why is networking slow in WSL2 on Windows 10?
    <a id="-why-is-networking-slow-in-wsl2-on-windows-10" href="#-why-is-networking-slow-in-wsl2-on-windows-10" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>This can impact both installing Ollama, as well as downloading models.</p>
<p>Open <code>Control Panel &gt; Networking and Internet &gt; View network status and tasks</code> and click on <code>Change adapter settings</code> on the left panel. Find the <code>vEthernel (WSL)</code> adapter, right click and select <code>Properties</code>.
Click on <code>Configure</code> and open the <code>Advanced</code> tab. Search through each of the properties until you find <code>Large Send Offload Version 2 (IPv4)</code> and <code>Large Send Offload Version 2 (IPv6)</code>. <em>Disable</em> both of these
properties.</p>




<h2 id="how-can-i-pre-load-a-model-to-get-faster-response-times" class="group block py-4">
    How can I pre-load a model to get faster response times?
    <a id="-how-can-i-pre-load-a-model-to-get-faster-response-times" href="#-how-can-i-pre-load-a-model-to-get-faster-response-times" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>If you are using the API you can preload a model by sending the Ollama server an empty request. This works with both the <code>/api/generate</code> and <code>/api/chat</code> API endpoints.</p>
<p>To preload the mistral model using the generate endpoint, use:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d <span style="color:#e6db74">&#39;{&#34;model&#34;: &#34;mistral&#34;}&#39;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

<p>To use the chat completions endpoint, use:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/chat -d <span style="color:#e6db74">&#39;{&#34;model&#34;: &#34;mistral&#34;}&#39;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>





<h2 id="how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately" class="group block py-4">
    How do I keep a model loaded in memory or make it unload immediately?
    <a id="-how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately" href="#-how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately" class="hidden group-hover:inline text-current ml-2">#</a>
</h2><p>By default models are kept in memory for 5 minutes before being unloaded. This allows for quicker response times if you are making numerous requests to the LLM. You may, however, want to free up the memory before the 5 minutes have elapsed or keep the model loaded indefinitely. Use the <code>keep_alive</code> parameter with either the <code>/api/generate</code> and <code>/api/chat</code> API endpoints to control how long the model is left in memory.</p>
<p>The <code>keep_alive</code> parameter can be set to:</p>
<ul>
<li>a duration string (such as &ldquo;10m&rdquo; or &ldquo;24h&rdquo;)</li>
<li>a number in seconds (such as 3600)</li>
<li>any negative number which will keep the model loaded in memory (e.g. -1 or &ldquo;-1m&rdquo;)</li>
<li>&lsquo;0&rsquo; which will unload the model immediately after generating a response</li>
</ul>
<p>For example, to preload a model and leave it in memory use:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d <span style="color:#e6db74">&#39;{&#34;model&#34;: &#34;llama3&#34;, &#34;keep_alive&#34;: -1}&#39;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

<p>To unload the model and free up memory use:</p>
<div class="codeblock relative bg-zinc-800 rounded text-sm p-4">
    <div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>curl http://localhost:11434/api/generate -d <span style="color:#e6db74">&#39;{&#34;model&#34;: &#34;llama3&#34;, &#34;keep_alive&#34;: 0}&#39;</span></span></span></code></pre></div>
    <button class="copy-btn absolute top-2 right-2 bg-zinc-600 hover:bg-zinc-900 transition duration-300 text-white rounded px-2 py-1 text-xs focus:outline-none" onclick="copyCodeToClipboard(this)">Copy</button>
</div>

<p>Alternatively, you can change the amount of time all models are loaded into memory by setting the <code>OLLAMA_KEEP_ALIVE</code> environment variable when starting the Ollama server. The <code>OLLAMA_KEEP_ALIVE</code> variable uses the same parameter types as the <code>keep_alive</code> parameter types mentioned above. Refer to section explaining <a href="#how-do-i-configure-ollama-server">how to configure the Ollama server</a> to correctly set the environment variable.</p>
<p>If you wish to override the <code>OLLAMA_KEEP_ALIVE</code> setting, use the <code>keep_alive</code> API parameter with the <code>/api/generate</code> or <code>/api/chat</code> API endpoints.</p>

  </div>
  <div class="p-4">
    
<div class="mt-8 flex justify-between items-center">
    
        <a href="/development/" class="tile text-black py-2 px-4 rounded-r-md transition duration-300">  </a>
    

    
        <a href="/gpu/" class="tile text-black py-2 px-4 rounded-l-md transition duration-300">  </a>
    
</div>
  </div>

    </div><div id="searchResultsContainer" class="hidden w-full lg:w-3/5 p-4" data-productFilter="">
    
</div><aside id="sidebar-right" class="hidden lg:block w-60">
  
    <div id="tocContainer" class="hidden toc sticky top-10 pt-4 h-[calc(100vh-4rem)] overflow-y-auto text-sm text-black transition duration-300"> 
    <nav id="TableOfContents">
  <ul>
    <li><a href="#how-can-i-upgrade-ollama">How can I upgrade Ollama?</a></li>
    <li><a href="#how-can-i-view-the-logs">How can I view the logs?</a></li>
    <li><a href="#is-my-gpu-compatible-with-ollama">Is my GPU compatible with Ollama?</a></li>
    <li><a href="#how-can-i-specify-the-context-window-size">How can I specify the context window size?</a></li>
    <li><a href="#how-do-i-configure-ollama-server">How do I configure Ollama server?</a>
      <ul>
        <li><a href="#setting-environment-variables-on-mac">Setting environment variables on Mac</a></li>
        <li><a href="#setting-environment-variables-on-linux">Setting environment variables on Linux</a></li>
        <li><a href="#setting-environment-variables-on-windows">Setting environment variables on Windows</a></li>
      </ul>
    </li>
    <li><a href="#how-can-i-expose-ollama-on-my-network">How can I expose Ollama on my network?</a></li>
    <li><a href="#how-can-i-use-ollama-with-a-proxy-server">How can I use Ollama with a proxy server?</a></li>
    <li><a href="#how-can-i-use-ollama-with-ngrok">How can I use Ollama with ngrok?</a></li>
    <li><a href="#how-can-i-use-ollama-with-cloudflare-tunnel">How can I use Ollama with Cloudflare Tunnel?</a></li>
    <li><a href="#how-can-i-allow-additional-web-origins-to-access-ollama">How can I allow additional web origins to access Ollama?</a></li>
    <li><a href="#where-are-models-stored">Where are models stored?</a>
      <ul>
        <li><a href="#how-do-i-set-them-to-a-different-location">How do I set them to a different location?</a></li>
      </ul>
    </li>
    <li><a href="#does-ollama-send-my-prompts-and-answers-back-to-ollamacom">Does Ollama send my prompts and answers back to ollama.com?</a></li>
    <li><a href="#how-can-i-use-ollama-in-visual-studio-code">How can I use Ollama in Visual Studio Code?</a></li>
    <li><a href="#how-do-i-use-ollama-behind-a-proxy">How do I use Ollama behind a proxy?</a>
      <ul>
        <li><a href="#how-do-i-use-ollama-behind-a-proxy-in-docker">How do I use Ollama behind a proxy in Docker?</a></li>
      </ul>
    </li>
    <li><a href="#how-do-i-use-ollama-with-gpu-acceleration-in-docker">How do I use Ollama with GPU acceleration in Docker?</a></li>
    <li><a href="#why-is-networking-slow-in-wsl2-on-windows-10">Why is networking slow in WSL2 on Windows 10?</a></li>
    <li><a href="#how-can-i-pre-load-a-model-to-get-faster-response-times">How can I pre-load a model to get faster response times?</a></li>
    <li><a href="#how-do-i-keep-a-model-loaded-in-memory-or-make-it-unload-immediately">How do I keep a model loaded in memory or make it unload immediately?</a></li>
  </ul>
</nav>
</div>
  
  
    <div id="chatContainer" class="hidden sticky top-16 h-[calc(100vh-5rem)] flex flex-col flex justify-end">
    <div id="chat-messages" class="flex flex-col overflow-y-auto text-base">
    </div>
    <div id="chat-controls" class="flex flex-row text-xs mt-2">
        <form id="chat-form" class="flex flex-row">
            <input id="question" type="text" aria-label="Question Input" placeholder="Ask the docs" class="h-10 border rounded-lg p-1 mr-1 focus:outline-none focus:ring-2 focus:ring-brand" />
            <button id="sendButton" aria-label="Send" class="flex items-center bg-brand my-1  hover:bg-black text-white p-1 mr-1 rounded-lg shadow-lg transition duration-300"><img src="/icons/send.svg" alt="Send" class="w-5 h-5"></button>
        </form>
        <button id="clearAll" aria-label="Delete All"  class="flex items-center bg-black my-1 hover:bg-red-600 text-white p-1 rounded-lg shadow-lg transition duration-300"><img src="/icons/delete.svg" alt="Delete" class="w-5 h-5"></button>
    </div>
</div>
  
</aside>
</main>
  <footer>


<script src="https://cdn.jsdelivr.net/npm/algoliasearch@latest/dist/algoliasearch-lite.umd.js" defer></script>
</footer>
</body>
</html>
