{
    "id": "0e1a79a7a5b946c20b7cbae5d0a5c544",
    "title": "Import",
    "description":"no description",
    "lastCommit": "0001-01-01 00:00:00 +0000 UTC",
    "version": "<no value>.<no value>.<no value>",
    "section":"no section",
    "parent": "MiloDocs",
    "isPage":true,
    "isSection":false,
    "pageKind":"page",
    "bundleType":"",
    "uri": "//localhost:1313/import/",
    "relURI": "/import/",
    "body": " Import a model # This guide walks through importing a GGUF, PyTorch or Safetensors model.\nImporting (GGUF) # Step 1: Write a Modelfile # Start by creating a Modelfile. This file is the blueprint for your model, specifying weights, parameters, prompt templates and more.\nFROM ./mistral-7b-v0.1.Q4_0.gguf Copy (Optional) many chat models require a prompt template in order to answer correctly. A default prompt template can be specified with the TEMPLATE instruction in the Modelfile:\nFROM ./mistral-7b-v0.1.Q4_0.gguf TEMPLATE \u0026#34;[INST] {{ .Prompt }} [/INST]\u0026#34; Copy Step 2: Create the Ollama model # Finally, create a model from your Modelfile:\nollama create example -f Modelfile Copy Step 3: Run your model # Next, test the model with ollama run:\nollama run example \u0026#34;What is your favourite condiment?\u0026#34; Copy Importing (PyTorch \u0026amp; Safetensors) # Importing from PyTorch and Safetensors is a longer process than importing from GGUF. Improvements that make it easier are a work in progress.\nSetup # First, clone the ollama/ollama repo:\ngit clone git@github.com:ollama/ollama.git ollama cd ollama Copy and then fetch its llama.cpp submodule:\ngit submodule init git submodule update llm/llama.cpp Copy Next, install the Python dependencies:\npython3 -m venv llm/llama.cpp/.venv source llm/llama.cpp/.venv/bin/activate pip install -r llm/llama.cpp/requirements.txt Copy Then build the quantize tool:\nmake -C llm/llama.cpp quantize Copy Clone the HuggingFace repository (optional) # If the model is currently hosted in a HuggingFace repository, first clone that repository to download the raw model.\nInstall Git LFS, verify it\u0026rsquo;s installed, and then clone the model\u0026rsquo;s repository:\ngit lfs install git clone https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1 model Copy Convert the model # Note: some model architectures require using specific convert scripts. For example, Qwen models require running convert-hf-to-gguf.py instead of convert.py\npython llm/llama.cpp/convert.py ./model --outtype f16 --outfile converted.bin Copy Quantize the model # llm/llama.cpp/quantize converted.bin quantized.bin q4_0 Copy Step 3: Write a Modelfile # Next, create a Modelfile for your model:\nFROM quantized.bin TEMPLATE \u0026#34;[INST] {{ .Prompt }} [/INST]\u0026#34; Copy Step 4: Create the Ollama model # Finally, create a model from your Modelfile:\nollama create example -f Modelfile Copy Step 5: Run your model # Next, test the model with ollama run:\nollama run example \u0026#34;What is your favourite condiment?\u0026#34; Copy Publishing your model (optional â€“ early alpha) # Publishing models is in early alpha. If you\u0026rsquo;d like to publish your model to share with others, follow these steps:\nCreate an account Copy your Ollama public key: macOS: cat ~/.ollama/id_ed25519.pub Windows: type %USERPROFILE%\\.ollama\\id_ed25519.pub Linux: cat /usr/share/ollama/.ollama/id_ed25519.pub Add your public key to your Ollama account Next, copy your model to your username\u0026rsquo;s namespace:\nollama cp example \u0026lt;your username\u0026gt;/example Copy Then push the model:\nollama push \u0026lt;your username\u0026gt;/example Copy After publishing, your model will be available at https://ollama.com/\u0026lt;your username\u0026gt;/example.\nQuantization reference # The quantization options are as follow (from highest highest to lowest levels of quantization). Note: some architectures such as Falcon do not support K quants.\nq2_K q3_K q3_K_S q3_K_M q3_K_L q4_0 (recommended) q4_1 q4_K q4_K_S q4_K_M q5_0 q5_1 q5_K q5_K_S q5_K_M q6_K q8_0 f16 ",
    "tags": []
}